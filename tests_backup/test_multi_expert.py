#!/usr/bin/env python3
"""
Test Multi-Expert Evaluation System
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "."))

from app.models import EvaluationConfig
from app.services.expert_evaluator import get_multi_expert_evaluator


def test_multi_expert_evaluator():
    """Test multi-expert evaluator with sample bacteriophage content"""

    print("ğŸ­ Testing Multi-Expert Evaluator System...")

    # Create evaluator
    config = EvaluationConfig(
        quality_threshold=0.7,
        evaluation_dimensions=["relevance", "completeness", "accuracy", "clarity", "coherence", "scientific_rigor"],
    )
    evaluator = get_multi_expert_evaluator(config)

    # Test content about bacteriophage therapy
    test_content = """
    å™¬èŒä½“æ²»ç–—æ˜¯ä¸€ç§åˆ›æ–°çš„æŠ—èŒç­–ç•¥ï¼Œåˆ©ç”¨å™¬èŒä½“çš„ç‰¹å¼‚æ€§æ¥é¶å‘æ€æ­»è‡´ç—…ç»†èŒã€‚
    è¿™ç§æ²»ç–—æ–¹æ³•åœ¨å¯¹æŠ—å¤šé‡è€è¯èŒæ„ŸæŸ“æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚
    
    å™¬èŒä½“å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
    1. é«˜åº¦ç‰¹å¼‚æ€§ï¼Œä¸ä¼šå½±å“æ­£å¸¸èŒç¾¤
    2. èƒ½å¤Ÿè¿›åŒ–ä»¥å¯¹æŠ—ç»†èŒè€è¯æ€§
    3. å‰¯ä½œç”¨ç›¸å¯¹è¾ƒå°‘
    
    åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼Œå™¬èŒä½“æ²»ç–—éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
    - æ‚£è€…å…ç–«ååº”
    - å™¬èŒä½“çš„ç¨³å®šæ€§å’Œæ´»æ€§
    - ç»™è¯é€”å¾„å’Œå‰‚é‡ä¼˜åŒ–
    
    ç›®å‰å¤šé¡¹ä¸´åºŠè¯•éªŒæ­£åœ¨è¿›è¡Œä¸­ï¼Œåˆæ­¥ç»“æœæ˜¾ç¤ºå™¬èŒä½“æ²»ç–—åœ¨æ²»ç–—é“œç»¿å‡å•èƒèŒã€
    é‡‘é»„è‰²è‘¡è„çƒèŒç­‰æ„ŸæŸ“æ–¹é¢å…·æœ‰è‰¯å¥½çš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚
    """

    task_context = {"name": "è¯„ä¼°å™¬èŒä½“æ²»ç–—çš„ä¸´åºŠåº”ç”¨å‰æ™¯å’ŒæŒ‘æˆ˜", "task_type": "clinical_analysis"}

    print(f"ğŸ“ æµ‹è¯•å†…å®¹é•¿åº¦: {len(test_content)} å­—ç¬¦")
    print(f"ğŸ¯ ä»»åŠ¡èƒŒæ™¯: {task_context['name']}")

    # Test with selected experts
    selected_experts = ["theoretical_biologist", "clinical_physician", "regulatory_expert"]
    print(f"ğŸ‘¥ é€‰æ‹©çš„ä¸“å®¶: {', '.join(selected_experts)}")

    try:
        # Multi-expert evaluation
        result = evaluator.evaluate_with_multiple_experts(
            content=test_content, task_context=task_context, selected_experts=selected_experts, iteration=1
        )

        print("\nğŸ­ å¤šä¸“å®¶è¯„ä¼°ç»“æœ:")

        # Individual expert results
        expert_evals = result.get("expert_evaluations", {})
        print(f"   æˆåŠŸè¯„ä¼°ä¸“å®¶æ•°: {len(expert_evals)}")

        for expert_name, evaluation in expert_evals.items():
            print(f"\n   ğŸ‘¤ {evaluation.get('expert_role', expert_name)}:")
            print(f"      æ€»ä½“è¯„åˆ†: {evaluation.get('overall_score', 0):.3f}")
            print(f"      ç›¸å…³æ€§: {evaluation.get('relevance', 0):.3f}")
            print(f"      å®Œæ•´æ€§: {evaluation.get('completeness', 0):.3f}")
            print(f"      å‡†ç¡®æ€§: {evaluation.get('accuracy', 0):.3f}")
            print(f"      å®ç”¨æ€§: {evaluation.get('practicality', 0):.3f}")
            print(f"      åˆ›æ–°æ€§: {evaluation.get('innovation', 0):.3f}")
            print(f"      é£é™©è¯„ä¼°: {evaluation.get('risk_assessment', 0):.3f}")
            print(f"      ä¿¡å¿ƒåº¦: {evaluation.get('confidence_level', 0):.3f}")

            # Show key insights
            strengths = evaluation.get("key_strengths", [])
            concerns = evaluation.get("major_concerns", [])
            suggestions = evaluation.get("specific_suggestions", [])

            if strengths:
                print(f"      âœ… ä¸»è¦ä¼˜åŠ¿: {strengths[0] if strengths else 'æ— '}")
            if concerns:
                print(f"      âš ï¸  ä¸»è¦å…³åˆ‡: {concerns[0] if concerns else 'æ— '}")
            if suggestions:
                print(f"      ğŸ’¡ æ”¹è¿›å»ºè®®: {suggestions[0] if suggestions else 'æ— '}")

        # Consensus results
        consensus = result.get("consensus", {})
        print(f"\nğŸ¤ ä¸“å®¶å…±è¯†:")
        print(f"   ç»¼åˆè¯„åˆ†: {consensus.get('overall_score', 0):.3f}")
        print(f"   å…±è¯†ä¿¡å¿ƒåº¦: {consensus.get('consensus_confidence', 0):.3f}")
        print(f"   å‚ä¸ä¸“å®¶æ•°: {consensus.get('expert_count', 0)}")

        # Disagreements
        disagreements = result.get("disagreements", [])
        if disagreements:
            print(f"\nğŸ”¥ ä¸“å®¶åˆ†æ­§:")
            for disagreement in disagreements:
                print(
                    f"   {disagreement['field']}: {disagreement['lowest_scorer']}({disagreement['lowest_score']:.2f}) vs {disagreement['highest_scorer']}({disagreement['highest_score']:.2f})"
                )
        else:
            print(f"\nâœ… ä¸“å®¶æ„è§ä¸€è‡´ï¼Œæ— é‡å¤§åˆ†æ­§")

        # Metadata
        metadata = result.get("metadata", {})
        print(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        print(f"   æˆåŠŸç‡: {metadata.get('successful_experts', 0)}/{metadata.get('total_experts', 0)}")
        print(f"   è¯„ä¼°æ–¹æ³•: {metadata.get('evaluation_method', 'unknown')}")

        # Success criteria
        success = (
            len(expert_evals) >= 2  # At least 2 experts evaluated
            and consensus.get("overall_score", 0) > 0.1  # Some meaningful score
            and consensus.get("consensus_confidence", 0) > 0.3  # Reasonable confidence
        )

        assert (
            success
        ), f"Multi-expert evaluation failed: experts={len(expert_evals)}, consensus_score={consensus.get('overall_score', 0):.3f}, confidence={consensus.get('consensus_confidence', 0):.3f}"

    except Exception as e:
        print(f"âŒ å¤šä¸“å®¶è¯„ä¼°å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Multi-expert evaluation failed with error: {e}"


def test_expert_roles():
    """Test individual expert role definitions"""

    print("\nğŸ” Testing Expert Role Definitions...")

    evaluator = get_multi_expert_evaluator()

    print(f"   ä¸“å®¶è§’è‰²æ•°é‡: {len(evaluator.experts)}")

    for name, expert in evaluator.experts.items():
        print(f"   ğŸ‘¤ {expert.name} (æƒé‡: {expert.weight})")
        print(f"      æè¿°: {expert.description}")
        print(f"      å…³æ³¨é¢†åŸŸ: {', '.join(expert.focus_areas)}")
        print()


if __name__ == "__main__":
    print("ğŸ­ Multi-Expert Evaluation System Test")
    print("=" * 50)

    try:
        # Test expert role definitions
        test_expert_roles()

        # Test multi-expert evaluation
        test_multi_expert_evaluator()

        print("\n" + "=" * 50)
        print("ğŸ¯ æµ‹è¯•ç»“æœ: âœ… æˆåŠŸ")

    except AssertionError as e:
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: âŒ å¤±è´¥ - {e}")
    except Exception as e:
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: âŒ é”™è¯¯ - {e}")

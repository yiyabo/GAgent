#!/usr/bin/env python3
"""
Quick test for LLM Evaluator
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "."))

from app.models import EvaluationConfig
from app.services.llm_evaluator import get_llm_evaluator


def test_llm_evaluator():
    """Test LLM evaluator with sample content"""

    print("ğŸ§ª Testing LLM Evaluator...")

    # Create evaluator
    config = EvaluationConfig(
        quality_threshold=0.7,
        evaluation_dimensions=["relevance", "completeness", "accuracy", "clarity", "coherence", "scientific_rigor"],
    )
    evaluator = get_llm_evaluator(config)

    # Test content
    test_content = """
    å™¬èŒä½“æ²»ç–—æ˜¯ä¸€ç§åˆ©ç”¨å™¬èŒä½“(bacteriophage)æ¥å¯¹æŠ—ç»†èŒæ„ŸæŸ“çš„æ–°å…´æ²»ç–—æ–¹æ³•ã€‚
    å™¬èŒä½“æ˜¯ä¸€ç§èƒ½å¤Ÿæ„ŸæŸ“å’Œæ€æ­»ç»†èŒçš„ç—…æ¯’ï¼Œå…·æœ‰é«˜åº¦çš„ç‰¹å¼‚æ€§ã€‚
    è¿™ç§æ²»ç–—æ–¹æ³•åœ¨æŠ—ç”Ÿç´ è€è¯æ€§æ—¥ç›Šä¸¥é‡çš„ä»Šå¤©æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚
    ç ”ç©¶è¡¨æ˜ï¼Œå™¬èŒä½“æ²»ç–—å¯ä»¥æœ‰æ•ˆåœ°æ²»ç–—å¤šç§ç»†èŒæ„ŸæŸ“ï¼ŒåŒ…æ‹¬é“œç»¿å‡å•èƒèŒæ„ŸæŸ“ã€‚
    """

    task_context = {"name": "ä»‹ç»å™¬èŒä½“æ²»ç–—çš„åŸºæœ¬åŸç†å’Œåº”ç”¨å‰æ™¯", "task_type": "content_generation"}

    print(f"ğŸ“ æµ‹è¯•å†…å®¹: {test_content[:50]}...")
    print(f"ğŸ¯ ä»»åŠ¡èƒŒæ™¯: {task_context['name']}")

    try:
        # Evaluate
        result = evaluator.evaluate_content_intelligent(test_content, task_context, 1)

        print("\nâœ… LLM è¯„ä¼°ç»“æœ:")
        print(f"   æ€»ä½“è¯„åˆ†: {result.overall_score:.3f}")
        print(f"   ç›¸å…³æ€§: {result.dimensions.relevance:.3f}")
        print(f"   å®Œæ•´æ€§: {result.dimensions.completeness:.3f}")
        print(f"   å‡†ç¡®æ€§: {result.dimensions.accuracy:.3f}")
        print(f"   æ¸…æ™°åº¦: {result.dimensions.clarity:.3f}")
        print(f"   è¿è´¯æ€§: {result.dimensions.coherence:.3f}")
        print(f"   ç§‘å­¦ä¸¥è°¨æ€§: {result.dimensions.scientific_rigor:.3f}")

        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, suggestion in enumerate(result.suggestions, 1):
            print(f"   {i}. {suggestion}")

        print(f"\nğŸ”„ éœ€è¦ä¿®è®¢: {'æ˜¯' if result.needs_revision else 'å¦'}")
        print(f"â° è¯„ä¼°æ–¹æ³•: {result.metadata.get('evaluation_method', 'unknown')}")

        assert result.overall_score > 0.5, f"LLM evaluation score too low: {result.overall_score:.3f}"

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        assert False, f"LLM evaluation failed with error: {e}"


if __name__ == "__main__":
    try:
        test_llm_evaluator()
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: âœ… æˆåŠŸ")
    except AssertionError as e:
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: âŒ å¤±è´¥ - {e}")
    except Exception as e:
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ: âŒ é”™è¯¯ - {e}")

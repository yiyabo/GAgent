#!/usr/bin/env python3
"""
Test Adversarial Evaluation System
"""

import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "."))

from app.models import EvaluationConfig
from app.services.adversarial_evaluator import get_adversarial_evaluator


def test_adversarial_evaluator():
    """Test adversarial evaluator with generator vs critic mechanism"""

    print("ğŸ¥Š Testing Adversarial Evaluation System...")

    # Create evaluator
    config = EvaluationConfig(quality_threshold=0.8, max_iterations=3)
    evaluator = get_adversarial_evaluator(config)

    # Test content with deliberate flaws
    flawed_content = """
    å™¬èŒä½“æ˜¯å¾ˆå¥½çš„ä¸œè¥¿ã€‚å®ƒä»¬å¯ä»¥æ€æ­»ç»†èŒã€‚
    
    å™¬èŒä½“å¾ˆå°ï¼Œæ¯”ç»†èŒå°å¾ˆå¤šã€‚å®ƒä»¬æ˜¯ç—…æ¯’ã€‚
    
    ä½¿ç”¨å™¬èŒä½“æ²»ç–—æ„ŸæŸ“æ˜¯æ–°æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•å¾ˆæœ‰å‰æ™¯ã€‚
    """

    task_context = {"name": "è¯¦ç»†ä»‹ç»å™¬èŒä½“æ²»ç–—çš„æœºåˆ¶ã€ä¼˜åŠ¿ã€æŒ‘æˆ˜å’Œåº”ç”¨å‰æ™¯", "task_type": "academic_review"}

    print(f"ğŸ“ æµ‹è¯•å†…å®¹ï¼ˆæ•…æ„åŒ…å«ç¼ºé™·ï¼‰:")
    print(f"   é•¿åº¦: {len(flawed_content)} å­—ç¬¦")
    print(f"   è¯æ•°: {len(flawed_content.split())} è¯")
    print(f"ğŸ¯ ä»»åŠ¡: {task_context['name']}")

    try:
        # Run adversarial evaluation
        print(f"\nâš”ï¸ å¼€å§‹å¯¹æŠ—æ€§è¯„ä¼°...")

        result = evaluator.adversarial_evaluate(
            content=flawed_content, task_context=task_context, max_rounds=3, improvement_threshold=0.1
        )

        print(f"\nâœ… å¯¹æŠ—æ€§è¯„ä¼°å®Œæˆ!")

        # Display results
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   æœ€ä½³é²æ£’æ€§è¯„åˆ†: {result['best_robustness_score']:.3f}")
        print(f"   å®Œæˆè½®æ¬¡: {result['rounds_completed']}/{result['total_rounds']}")
        print(f"   æ€»å‘ç°é—®é¢˜æ•°: {result['metadata']['total_criticisms']}")
        print(f"   å¹³å‡é²æ£’æ€§: {result['metadata']['average_robustness']:.3f}")

        # Show round details
        print(f"\nğŸ”„ å¯¹æŠ—è½®æ¬¡è¯¦æƒ…:")
        for i, round_data in enumerate(result["adversarial_rounds"], 1):
            print(f"   è½®æ¬¡ {i}:")
            print(f"      å‘ç°é—®é¢˜: {round_data['criticism_count']}")
            print(f"      é²æ£’æ€§è¯„åˆ†: {round_data['robustness_score']:.3f}")
            print(f"      å†…å®¹å·²æ”¹è¿›: {'æ˜¯' if round_data['improvement_made'] else 'å¦'}")

            # Show some criticisms
            if round_data["criticisms"]:
                print(f"      ä¸»è¦é—®é¢˜:")
                for j, criticism in enumerate(round_data["criticisms"][:2], 1):
                    issue = criticism.get("issue", "æœªçŸ¥é—®é¢˜")
                    severity = criticism.get("severity", "æœªçŸ¥")
                    print(f"        {j}. [{severity}] {issue}")

        # Final assessment
        assessment = result["final_assessment"]
        print(f"\nğŸ¯ æœ€ç»ˆè¯„ä¼°:")
        print(f"   æœ€ç»ˆé²æ£’æ€§è¯„åˆ†: {assessment['final_robustness_score']:.3f}")
        print(f"   æ€»æ‰¹è¯„æ•°: {assessment['total_criticisms_found']}")
        print(f"   å¯¹æŠ—æœ‰æ•ˆæ€§: {assessment['adversarial_effectiveness']:.3f}")
        print(f"   æ”¶æ•›è¾¾æˆ: {'æ˜¯' if assessment['convergence_achieved'] else 'å¦'}")
        print(f"   æ¨è: {assessment['recommendation']}")

        # Most common issues
        if assessment["most_common_issues"]:
            print(f"\nğŸ” æœ€å¸¸è§é—®é¢˜ç±»å‹:")
            for category, count in assessment["most_common_issues"]:
                print(f"      {category}: {count}æ¬¡")

        # Show improvement trend
        if assessment["improvement_trend"]:
            print(f"\nğŸ“ˆ æ”¹è¿›è¶‹åŠ¿:")
            for i, improvement in enumerate(assessment["improvement_trend"], 1):
                direction = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
                print(f"      è½®æ¬¡ {i+1}: {direction} {improvement:+.3f}")

        # Content comparison
        print(f"\nğŸ“ å†…å®¹å¯¹æ¯”:")
        print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(flawed_content)} å­—ç¬¦")
        print(f"   æœ€ä½³å†…å®¹é•¿åº¦: {len(result['best_content'])} å­—ç¬¦")

        if len(result["best_content"]) > len(flawed_content):
            print(f"   âœ… å†…å®¹å¾—åˆ°äº†æ‰©å±•å’Œæ”¹è¿›")

        # Success criteria
        success = (
            result["rounds_completed"] >= 1
            and result["best_robustness_score"] > 0.3  # Some improvement
            and result["metadata"]["total_criticisms"] > 0  # Critic found issues
        )

        assert (
            success
        ), f"Adversarial evaluation failed: rounds={result['rounds_completed']}, score={result['best_robustness_score']:.3f}, criticisms={result['metadata']['total_criticisms']}"

    except Exception as e:
        print(f"âŒ å¯¹æŠ—æ€§è¯„ä¼°å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_generator_and_critic_separately():
    """Test generator and critic components individually"""

    print("\nğŸ” Testing Individual Components...")

    evaluator = get_adversarial_evaluator()

    # Test critic
    print("\nğŸ‘¤ Testing Content Critic:")
    test_content = "å™¬èŒä½“æ²»ç–—å¾ˆå¥½ã€‚"
    task_context = {"name": "å™¬èŒä½“æ²»ç–—æœºåˆ¶åˆ†æ"}

    criticisms = evaluator.critic.critique_content(test_content, task_context)
    print(f"   å‘ç°é—®é¢˜æ•°: {len(criticisms)}")

    if criticisms:
        for i, criticism in enumerate(criticisms[:2], 1):
            print(f"   é—®é¢˜ {i}: {criticism.get('issue', 'æœªçŸ¥')}")

    # Test generator
    print("\nâš™ï¸ Testing Content Generator:")
    if criticisms:
        improved = evaluator.generator.improve_content(test_content, criticisms, task_context)
        print(f"   åŸå§‹é•¿åº¦: {len(test_content)} å­—ç¬¦")
        print(f"   æ”¹è¿›é•¿åº¦: {len(improved)} å­—ç¬¦")
        print(f"   å†…å®¹å·²æ”¹è¿›: {'æ˜¯' if improved != test_content else 'å¦'}")

    assert len(criticisms) > 0, "Content critic should find at least one issue with the test content"


if __name__ == "__main__":
    print("ğŸ¥Š Adversarial Evaluation System Test")
    print("=" * 50)

    try:
        # Test individual components
        test_generator_and_critic_separately()
        print("   ç»„ä»¶æµ‹è¯•: âœ… æˆåŠŸ")

        print("\n" + "=" * 50)

        # Test full adversarial system
        test_adversarial_evaluator()
        print("   å¯¹æŠ—æ€§æµ‹è¯•: âœ… æˆåŠŸ")

        print("\n" + "=" * 50)
        print("ğŸ¯ æ€»ä½“ç»“æœ: âœ… æˆåŠŸ")

    except AssertionError as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•é”™è¯¯: {e}")

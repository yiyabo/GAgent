#!/usr/bin/env python3
"""
è¯„ä¼°ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡ä»¶åŒ…å«äº†å„ç§è¯„ä¼°æ¨¡å¼çš„å®é™…ä½¿ç”¨ç¤ºä¾‹ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹å’Œç†è§£ç³»ç»ŸåŠŸèƒ½ã€‚
"""

import sys
import os
import time
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from app.execution.executors.enhanced import (
    execute_task_with_evaluation,
    execute_task_with_llm_evaluation,
    execute_task_with_multi_expert_evaluation,
    execute_task_with_adversarial_evaluation
)
from app.models import EvaluationConfig
from app.repository.tasks import default_repo
from app.services.evaluation_supervisor import get_evaluation_supervisor, get_supervision_report
from app.services.evaluation_cache import get_evaluation_cache
from app.services.meta_evaluator import get_meta_evaluator
from app.services.phage_evaluator import get_phage_evaluator


def example_basic_evaluation():
    """ç¤ºä¾‹1: åŸºç¡€è¯„ä¼°"""
    print("=" * 60)
    print("ç¤ºä¾‹1: åŸºç¡€è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹ä»»åŠ¡
    task = {
        "id": 1001,
        "name": "ç¼–å†™å™¬èŒä½“æ²»ç–—çš„åŸºç¡€ä»‹ç»",
        "content": ""
    }
    
    # é…ç½®è¯„ä¼°å‚æ•°
    config = EvaluationConfig(
        quality_threshold=0.7,
        max_iterations=3,
        evaluation_dimensions=["relevance", "completeness", "accuracy", "clarity"]
    )
    
    print(f"ä»»åŠ¡: {task['name']}")
    print(f"è´¨é‡é˜ˆå€¼: {config.quality_threshold}")
    print(f"æœ€å¤§è¿­ä»£: {config.max_iterations}")
    print()
    
    try:
        # æ‰§è¡ŒåŸºç¡€è¯„ä¼°
        start_time = time.time()
        result = execute_task_with_evaluation(
            task=task,
            repo=default_repo,
            max_iterations=config.max_iterations,
            quality_threshold=config.quality_threshold,
            evaluation_config=config
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("âœ… åŸºç¡€è¯„ä¼°å®Œæˆ!")
        print(f"æœ€ç»ˆçŠ¶æ€: {result.status}")
        print(f"æœ€ç»ˆè¯„åˆ†: {result.evaluation.overall_score:.3f}")
        print(f"å®Œæˆè¿­ä»£: {result.iterations}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        if result.evaluation.suggestions:
            print("\næ”¹è¿›å»ºè®®:")
            for i, suggestion in enumerate(result.evaluation.suggestions[:3], 1):
                print(f"  {i}. {suggestion}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åŸºç¡€è¯„ä¼°å¤±è´¥: {e}")
        return None


def example_llm_intelligent_evaluation():
    """ç¤ºä¾‹2: LLMæ™ºèƒ½è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹2: LLMæ™ºèƒ½è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹ä»»åŠ¡
    task = {
        "id": 1002,
        "name": "åˆ†æå™¬èŒä½“åœ¨æŠ—ç”Ÿç´ è€è¯æ€§æ²»ç–—ä¸­çš„åº”ç”¨å‰æ™¯",
        "content": ""
    }
    
    # é…ç½®LLMè¯„ä¼°å‚æ•°
    config = EvaluationConfig(
        quality_threshold=0.8,
        max_iterations=3,
        strict_mode=True,
        evaluation_dimensions=["relevance", "completeness", "accuracy", "clarity", "coherence", "scientific_rigor"]
    )
    
    print(f"ä»»åŠ¡: {task['name']}")
    print(f"è¯„ä¼°æ¨¡å¼: LLMæ™ºèƒ½è¯„ä¼°")
    print(f"è´¨é‡é˜ˆå€¼: {config.quality_threshold}")
    print(f"ä¸¥æ ¼æ¨¡å¼: {config.strict_mode}")
    print()
    
    try:
        # æ‰§è¡ŒLLMæ™ºèƒ½è¯„ä¼°
        start_time = time.time()
        result = execute_task_with_llm_evaluation(
            task=task,
            repo=default_repo,
            max_iterations=config.max_iterations,
            quality_threshold=config.quality_threshold,
            evaluation_config=config,
            use_context=True
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print("ğŸ§  LLMæ™ºèƒ½è¯„ä¼°å®Œæˆ!")
        print(f"æœ€ç»ˆçŠ¶æ€: {result.status}")
        print(f"æœ€ç»ˆè¯„åˆ†: {result.evaluation.overall_score:.3f}")
        print(f"å®Œæˆè¿­ä»£: {result.iterations_completed}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ˜¾ç¤ºç»´åº¦è¯„åˆ†
        print("\nğŸ“Š ç»´åº¦è¯„åˆ†:")
        dimensions = result.evaluation.dimensions
        print(f"  ç›¸å…³æ€§: {dimensions.relevance:.3f}")
        print(f"  å®Œæ•´æ€§: {dimensions.completeness:.3f}")
        print(f"  å‡†ç¡®æ€§: {dimensions.accuracy:.3f}")
        print(f"  æ¸…æ™°åº¦: {dimensions.clarity:.3f}")
        print(f"  è¿è´¯æ€§: {dimensions.coherence:.3f}")
        print(f"  ç§‘å­¦ä¸¥è°¨æ€§: {dimensions.scientific_rigor:.3f}")
        
        # æ˜¾ç¤ºæ™ºèƒ½å»ºè®®
        if result.evaluation.suggestions:
            print("\nğŸ’¡ æ™ºèƒ½æ”¹è¿›å»ºè®®:")
            for i, suggestion in enumerate(result.evaluation.suggestions[:3], 1):
                print(f"  {i}. {suggestion}")
        
        return result
        
    except Exception as e:
        print(f"âŒ LLMæ™ºèƒ½è¯„ä¼°å¤±è´¥: {e}")
        return None


def example_multi_expert_evaluation():
    """ç¤ºä¾‹3: å¤šä¸“å®¶è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹3: å¤šä¸“å®¶è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹ä»»åŠ¡
    task = {
        "id": 1003,
        "name": "è¯„ä¼°å™¬èŒä½“ç–—æ³•çš„ä¸´åºŠè¯•éªŒè®¾è®¡æ–¹æ¡ˆ",
        "content": ""
    }
    
    # é€‰æ‹©ç‰¹å®šä¸“å®¶
    selected_experts = ["theoretical_biologist", "clinical_physician", "regulatory_expert"]
    
    print(f"ä»»åŠ¡: {task['name']}")
    print(f"è¯„ä¼°æ¨¡å¼: å¤šä¸“å®¶åä½œè¯„ä¼°")
    print(f"å‚ä¸ä¸“å®¶: {', '.join(selected_experts)}")
    print()
    
    try:
        # æ‰§è¡Œå¤šä¸“å®¶è¯„ä¼°
        start_time = time.time()
        result = execute_task_with_multi_expert_evaluation(
            task=task,
            repo=default_repo,
            max_iterations=3,
            quality_threshold=0.8,
            selected_experts=selected_experts,
            use_context=True
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("ğŸ­ å¤šä¸“å®¶è¯„ä¼°å®Œæˆ!")
        print(f"æœ€ç»ˆçŠ¶æ€: {result.status}")
        print(f"ä¸“å®¶å…±è¯†è¯„åˆ†: {result.evaluation.overall_score:.3f}")
        print(f"å®Œæˆè¿­ä»£: {result.iterations_completed}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ˜¾ç¤ºä¸“å®¶è¯¦æƒ…
        metadata = result.metadata or {}
        expert_evaluations = metadata.get('expert_evaluations', {})
        disagreements = metadata.get('disagreements', [])
        consensus_confidence = metadata.get('consensus_confidence', 0.0)
        
        if expert_evaluations:
            print("\nğŸ‘¥ å„ä¸“å®¶è¯„åˆ†:")
            for expert_name, evaluation in expert_evaluations.items():
                expert_role = evaluation.get('expert_role', expert_name)
                overall_score = evaluation.get('overall_score', 0)
                confidence = evaluation.get('confidence_level', 0)
                print(f"  {expert_role}: {overall_score:.3f} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            print(f"\nğŸ¤ ä¸“å®¶å…±è¯†ç½®ä¿¡åº¦: {consensus_confidence:.3f}")
        
        # æ˜¾ç¤ºä¸“å®¶åˆ†æ­§
        if disagreements:
            print(f"\nğŸ”¥ ä¸“å®¶åˆ†æ­§ ({len(disagreements)} ä¸ªé¢†åŸŸ):")
            for disagreement in disagreements[:3]:
                field = disagreement['field']
                level = disagreement['disagreement_level']
                lowest = disagreement['lowest_scorer']
                highest = disagreement['highest_scorer']
                print(f"  {field}: {lowest} vs {highest} (åˆ†æ­§åº¦: {level:.2f})")
        else:
            print("\nâœ… ä¸“å®¶æ„è§é«˜åº¦ä¸€è‡´")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¤šä¸“å®¶è¯„ä¼°å¤±è´¥: {e}")
        return None


def example_adversarial_evaluation():
    """ç¤ºä¾‹4: å¯¹æŠ—æ€§è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹4: å¯¹æŠ—æ€§è¯„ä¼° (ç”Ÿæˆå™¨ vs æ‰¹è¯„è€…)")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹ä»»åŠ¡
    task = {
        "id": 1004,
        "name": "åˆ¶å®šå™¬èŒä½“æ²»ç–—çš„å®‰å…¨æ€§è¯„ä¼°æ ‡å‡†",
        "content": ""
    }
    
    print(f"ä»»åŠ¡: {task['name']}")
    print(f"è¯„ä¼°æ¨¡å¼: å¯¹æŠ—æ€§è¯„ä¼°")
    print(f"æœ€å¤§è½®æ•°: 3")
    print(f"æ”¹è¿›é˜ˆå€¼: 0.1")
    print()
    
    try:
        # æ‰§è¡Œå¯¹æŠ—æ€§è¯„ä¼°
        start_time = time.time()
        result = execute_task_with_adversarial_evaluation(
            task=task,
            repo=default_repo,
            max_rounds=3,
            improvement_threshold=0.1,
            use_context=True
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("âš”ï¸ å¯¹æŠ—æ€§è¯„ä¼°å®Œæˆ!")
        print(f"æœ€ç»ˆçŠ¶æ€: {result.status}")
        print(f"é²æ£’æ€§è¯„åˆ†: {result.evaluation.overall_score:.3f}")
        print(f"å®Œæˆè½®æ•°: {result.iterations_completed}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ˜¾ç¤ºå¯¹æŠ—æ€§åˆ†æ
        metadata = result.metadata or {}
        adversarial_effectiveness = metadata.get('adversarial_effectiveness', 0.0)
        robustness_score = metadata.get('robustness_score', 0.0)
        
        print(f"\nâš”ï¸ å¯¹æŠ—æ€§åˆ†æ:")
        print(f"  å¯¹æŠ—æ€§æ•ˆæœ: {adversarial_effectiveness:.3f}")
        print(f"  æœ€ç»ˆé²æ£’æ€§: {robustness_score:.3f}")
        
        # æ˜¾ç¤ºå¯¹æŠ—æ€§æ´å¯Ÿ
        if result.evaluation.suggestions:
            print("\nğŸ’¡ å¯¹æŠ—æ€§æ´å¯Ÿ:")
            for i, suggestion in enumerate(result.evaluation.suggestions[:3], 1):
                print(f"  {i}. {suggestion}")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¯¹æŠ—æ€§è¯„ä¼°å¤±è´¥: {e}")
        return None


def example_phage_domain_evaluation():
    """ç¤ºä¾‹5: å™¬èŒä½“ä¸“ä¸šé¢†åŸŸè¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹5: å™¬èŒä½“ä¸“ä¸šé¢†åŸŸè¯„ä¼°")
    print("=" * 60)
    
    # ç¤ºä¾‹å™¬èŒä½“ç ”ç©¶å†…å®¹
    phage_content = """
    å™¬èŒä½“ç–—æ³•æ˜¯ä¸€ç§åˆ©ç”¨å™¬èŒä½“ï¼ˆbacteriophageï¼‰æ¥æ²»ç–—ç»†èŒæ„ŸæŸ“çš„æ–°å…´æ²»ç–—æ–¹æ³•ã€‚
    å™¬èŒä½“æ˜¯ä¸“é—¨æ„ŸæŸ“ç»†èŒçš„ç—…æ¯’ï¼Œå…·æœ‰é«˜åº¦çš„å®¿ä¸»ç‰¹å¼‚æ€§ã€‚åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼Œ
    æˆ‘ä»¬éœ€è¦è€ƒè™‘å™¬èŒä½“çš„è£‚è§£å‘¨æœŸã€å®¿ä¸»èŒƒå›´ã€ä»¥åŠæ½œåœ¨çš„ç»†èŒè€è¯æ€§é—®é¢˜ã€‚
    
    å…³é”®çš„å®‰å…¨æ€§è€ƒè™‘åŒ…æ‹¬ï¼š
    1. å™¬èŒä½“çš„åŸºå› ç»„åˆ†æï¼Œç¡®ä¿ä¸å«æœ‰æ¯’åŠ›åŸºå› 
    2. å†…æ¯’ç´ æ°´å¹³çš„æ£€æµ‹å’Œæ§åˆ¶
    3. å…ç–«åŸæ€§è¯„ä¼°
    4. é•¿æœŸç¨³å®šæ€§ç ”ç©¶
    
    ç›‘ç®¡æ–¹é¢ï¼ŒFDAå·²ç»æ‰¹å‡†äº†ä¸€äº›å™¬èŒä½“äº§å“ç”¨äºé£Ÿå“å®‰å…¨ï¼Œ
    ä½†æ²»ç–—æ€§å™¬èŒä½“äº§å“ä»éœ€è¦æ›´ä¸¥æ ¼çš„ä¸´åºŠè¯•éªŒæ•°æ®ã€‚
    """
    
    task_context = {
        "name": "å™¬èŒä½“ç–—æ³•å®‰å…¨æ€§è¯„ä¼°",
        "research_focus": "therapeutic_applications",
        "target_audience": "clinical_researchers"
    }
    
    print(f"è¯„ä¼°å†…å®¹: å™¬èŒä½“ç–—æ³•ä¸“ä¸šæ–‡æ¡£")
    print(f"ç ”ç©¶é‡ç‚¹: {task_context['research_focus']}")
    print(f"ç›®æ ‡å—ä¼—: {task_context['target_audience']}")
    print()
    
    try:
        # æ‰§è¡Œå™¬èŒä½“ä¸“ä¸šè¯„ä¼°
        phage_evaluator = get_phage_evaluator()
        
        start_time = time.time()
        result = phage_evaluator.evaluate_phage_content(
            content=phage_content,
            task_context=task_context
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("ğŸ¦  å™¬èŒä½“ä¸“ä¸šè¯„ä¼°å®Œæˆ!")
        print(f"æ•´ä½“ä¸“ä¸šè¯„åˆ†: {result['overall_score']:.3f}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ˜¾ç¤ºä¸“ä¸šç»´åº¦è¯„åˆ†
        print("\nğŸ“Š ä¸“ä¸šç»´åº¦è¯„åˆ†:")
        print(f"  æœ¯è¯­å‡†ç¡®æ€§: {result['terminology_accuracy']:.3f}")
        print(f"  ä¸´åºŠç›¸å…³æ€§: {result['clinical_relevance']:.3f}")
        print(f"  å®‰å…¨æ€§è¯„ä¼°: {result['safety_assessment']:.3f}")
        print(f"  ç ”ç©¶æ–¹æ³•: {result['research_methodology']:.3f}")
        
        # æ˜¾ç¤ºä¸“ä¸šå»ºè®®
        if result.get('professional_suggestions'):
            print("\nğŸ’¡ ä¸“ä¸šå»ºè®®:")
            for i, suggestion in enumerate(result['professional_suggestions'][:3], 1):
                print(f"  {i}. {suggestion}")
        
        # æ˜¾ç¤ºæœ¯è¯­åˆ†æ
        terminology_analysis = result.get('terminology_analysis', {})
        if terminology_analysis:
            correct_terms = terminology_analysis.get('correct_terms', [])
            questionable_terms = terminology_analysis.get('questionable_terms', [])
            
            if correct_terms:
                print(f"\nâœ… æ­£ç¡®ä½¿ç”¨çš„ä¸“ä¸šæœ¯è¯­: {', '.join(correct_terms[:5])}")
            if questionable_terms:
                print(f"\nâš ï¸ éœ€è¦ç¡®è®¤çš„æœ¯è¯­: {', '.join(questionable_terms[:3])}")
        
        return result
        
    except Exception as e:
        print(f"âŒ å™¬èŒä½“ä¸“ä¸šè¯„ä¼°å¤±è´¥: {e}")
        return None


def example_meta_cognitive_evaluation():
    """ç¤ºä¾‹6: å…ƒè®¤çŸ¥è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹6: å…ƒè®¤çŸ¥è¯„ä¼° (è¯„ä¼°çš„è¯„ä¼°)")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè¯„ä¼°å†å²
    evaluation_history = [
        {
            "iteration": 1,
            "overall_score": 0.65,
            "dimension_scores": {"relevance": 0.7, "completeness": 0.6, "accuracy": 0.65},
            "suggestions": ["å¢åŠ æ›´å¤šç»†èŠ‚", "æ”¹è¿›é€»è¾‘ç»“æ„"],
            "timestamp": "2024-01-01T10:00:00"
        },
        {
            "iteration": 2,
            "overall_score": 0.75,
            "dimension_scores": {"relevance": 0.8, "completeness": 0.7, "accuracy": 0.75},
            "suggestions": ["å®Œå–„ç»“è®ºéƒ¨åˆ†", "æ·»åŠ å‚è€ƒæ–‡çŒ®"],
            "timestamp": "2024-01-01T10:05:00"
        },
        {
            "iteration": 3,
            "overall_score": 0.82,
            "dimension_scores": {"relevance": 0.85, "completeness": 0.8, "accuracy": 0.8},
            "suggestions": ["ä¼˜åŒ–è¡¨è¾¾æ–¹å¼"],
            "timestamp": "2024-01-01T10:10:00"
        }
    ]
    
    current_evaluation = evaluation_history[-1]
    task_context = {
        "name": "å™¬èŒä½“æ²»ç–—æœºåˆ¶ç ”ç©¶",
        "task_type": "scientific_analysis"
    }
    
    print(f"åˆ†æä»»åŠ¡: {task_context['name']}")
    print(f"è¯„ä¼°å†å²: {len(evaluation_history)} æ¬¡è¿­ä»£")
    print(f"è¯„åˆ†è¶‹åŠ¿: {evaluation_history[0]['overall_score']:.3f} â†’ {evaluation_history[-1]['overall_score']:.3f}")
    print()
    
    try:
        # æ‰§è¡Œå…ƒè®¤çŸ¥è¯„ä¼°
        meta_evaluator = get_meta_evaluator()
        
        start_time = time.time()
        result = meta_evaluator.meta_evaluate_assessment_quality(
            evaluation_history=evaluation_history,
            task_context=task_context,
            current_evaluation=current_evaluation
        )
        execution_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        print("ğŸ§  å…ƒè®¤çŸ¥è¯„ä¼°å®Œæˆ!")
        print(f"è¯„ä¼°è´¨é‡è¯„åˆ†: {result['assessment_quality_score']:.3f}")
        print(f"ä¸€è‡´æ€§è¯„åˆ†: {result['consistency_score']:.3f}")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        
        # æ˜¾ç¤ºè®¤çŸ¥åè§åˆ†æ
        bias_analysis = result['cognitive_bias_analysis']
        print("\nğŸ§  è®¤çŸ¥åè§é£é™©åˆ†æ:")
        high_risk_biases = []
        for bias_type, risk_level in bias_analysis.items():
            risk_status = "é«˜é£é™©" if risk_level > 0.6 else "ä¸­é£é™©" if risk_level > 0.3 else "ä½é£é™©"
            color = "ğŸ”´" if risk_level > 0.6 else "ğŸŸ¡" if risk_level > 0.3 else "ğŸŸ¢"
            print(f"  {color} {bias_type}: {risk_level:.2f} ({risk_status})")
            if risk_level > 0.6:
                high_risk_biases.append(bias_type)
        
        # æ˜¾ç¤ºå…ƒè®¤çŸ¥æ´å¯Ÿ
        meta_insights = result.get('meta_cognitive_insights', [])
        if meta_insights:
            print("\nğŸ’¡ å…ƒè®¤çŸ¥æ´å¯Ÿ:")
            for i, insight in enumerate(meta_insights[:3], 1):
                print(f"  {i}. {insight}")
        
        # é«˜é£é™©åè§è­¦å‘Š
        if high_risk_biases:
            print(f"\nâš ï¸ æ£€æµ‹åˆ°é«˜é£é™©è®¤çŸ¥åè§: {', '.join(high_risk_biases)}")
            print("   å»ºè®®é‡‡å–æªæ–½å‡å°‘åè§å½±å“")
        
        return result
        
    except Exception as e:
        print(f"âŒ å…ƒè®¤çŸ¥è¯„ä¼°å¤±è´¥: {e}")
        return None


def example_supervision_system():
    """ç¤ºä¾‹7: ç›‘ç£ç³»ç»Ÿæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹7: è¯„ä¼°è´¨é‡ç›‘ç£ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        # è·å–ç›‘ç£æŠ¥å‘Š
        print("ğŸ” è·å–ç³»ç»Ÿç›‘ç£æŠ¥å‘Š...")
        supervision_report = get_supervision_report()
        
        # æ˜¾ç¤ºç³»ç»Ÿå¥åº·çŠ¶æ€
        system_health = supervision_report.get("system_health", {})
        overall_score = system_health.get("overall_score", 0.0)
        status = system_health.get("status", "unknown")
        
        print(f"\nğŸ“Š ç³»ç»Ÿå¥åº·çŠ¶æ€:")
        print(f"  æ•´ä½“å¥åº·è¯„åˆ†: {overall_score:.3f}")
        print(f"  ç³»ç»ŸçŠ¶æ€: {status.upper()}")
        
        # æ˜¾ç¤ºå½“å‰è´¨é‡æŒ‡æ ‡
        current_metrics = supervision_report.get("current_metrics", {})
        if current_metrics:
            print(f"\nğŸ“ˆ å½“å‰è´¨é‡æŒ‡æ ‡:")
            for metric_name, metric_data in current_metrics.items():
                value = metric_data.get("value", 0.0)
                status = metric_data.get("status", "unknown")
                threshold = metric_data.get("threshold", 0.0)
                status_icon = "âœ…" if status == "good" else "âš ï¸" if status == "warning" else "âŒ"
                print(f"  {status_icon} {metric_name}: {value:.3f} (é˜ˆå€¼: {threshold:.3f})")
        
        # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
        performance_summary = supervision_report.get("performance_summary", {})
        if performance_summary:
            print(f"\nâš¡ æ€§èƒ½æ‘˜è¦:")
            avg_time = performance_summary.get("avg_evaluation_time", 0.0)
            success_rate = performance_summary.get("success_rate", 0.0)
            cache_hit_rate = performance_summary.get("avg_cache_hit_rate", 0.0)
            print(f"  å¹³å‡è¯„ä¼°æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"  æˆåŠŸç‡: {success_rate:.1%}")
            print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}")
        
        # æ˜¾ç¤ºæœ€è¿‘è­¦æŠ¥
        alert_summary = supervision_report.get("alert_summary", {})
        total_alerts = alert_summary.get("total", 0)
        critical_alerts = alert_summary.get("critical", 0)
        
        if total_alerts > 0:
            print(f"\nğŸš¨ æœ€è¿‘24å°æ—¶è­¦æŠ¥: {total_alerts} ä¸ª")
            if critical_alerts > 0:
                print(f"  å…¶ä¸­ä¸¥é‡è­¦æŠ¥: {critical_alerts} ä¸ª")
        else:
            print(f"\nâœ… æœ€è¿‘24å°æ—¶æ— è­¦æŠ¥")
        
        # æ¼”ç¤ºç›‘ç£é…ç½®
        print(f"\nğŸ”§ ç›‘ç£ç³»ç»Ÿé…ç½®:")
        supervisor = get_evaluation_supervisor()
        
        # æ›´æ–°ä¸€äº›é˜ˆå€¼ä½œä¸ºæ¼”ç¤º
        new_thresholds = {
            "min_accuracy": 0.75,
            "max_evaluation_time": 30.0
        }
        
        success = supervisor.update_thresholds(new_thresholds)
        if success:
            print(f"  âœ… æˆåŠŸæ›´æ–°ç›‘ç£é˜ˆå€¼:")
            for threshold_name, value in new_thresholds.items():
                print(f"    {threshold_name}: {value}")
        
        return supervision_report
        
    except Exception as e:
        print(f"âŒ ç›‘ç£ç³»ç»Ÿæ¼”ç¤ºå¤±è´¥: {e}")
        return None


def example_cache_optimization():
    """ç¤ºä¾‹8: ç¼“å­˜ç³»ç»Ÿä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹8: ç¼“å­˜ç³»ç»Ÿå’Œæ€§èƒ½ä¼˜åŒ–")
    print("=" * 60)
    
    try:
        # è·å–ç¼“å­˜å®ä¾‹
        cache = get_evaluation_cache()
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        print("ğŸ“Š ç¼“å­˜ç³»ç»ŸçŠ¶æ€:")
        stats = cache.get_cache_stats()
        print(f"  ç¼“å­˜å¤§å°: {stats.get('cache_size', 0)} æ¡ç›®")
        print(f"  å‘½ä¸­ç‡: {stats.get('hit_rate', 0.0):.1%}")
        print(f"  æ€»æŸ¥è¯¢æ•°: {stats.get('total_queries', 0)}")
        print(f"  ç¼“å­˜å‘½ä¸­æ•°: {stats.get('cache_hits', 0)}")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        performance_stats = cache.get_performance_stats()
        if performance_stats:
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {performance_stats.get('avg_query_time', 0.0):.3f}ms")
            print(f"  ç¼“å­˜æ•ˆç‡: {performance_stats.get('cache_efficiency', 0.0):.1%}")
        
        # æ¼”ç¤ºç¼“å­˜ä¼˜åŒ–
        print(f"\nğŸ”§ æ‰§è¡Œç¼“å­˜ä¼˜åŒ–...")
        optimization_result = cache.optimize_cache()
        
        print(f"  æ¸…ç†è¿‡æœŸæ¡ç›®: {optimization_result.get('entries_removed', 0)} ä¸ª")
        print(f"  é‡Šæ”¾å†…å­˜: {optimization_result.get('memory_freed', 0)} bytes")
        print(f"  ä¼˜åŒ–åç¼“å­˜å¤§å°: {optimization_result.get('final_cache_size', 0)} æ¡ç›®")
        
        # æ˜¾ç¤ºä¼˜åŒ–åçš„ç»Ÿè®¡
        new_stats = cache.get_cache_stats()
        print(f"\nğŸ“ˆ ä¼˜åŒ–åç¼“å­˜çŠ¶æ€:")
        print(f"  ç¼“å­˜å¤§å°: {new_stats.get('cache_size', 0)} æ¡ç›®")
        print(f"  å‘½ä¸­ç‡: {new_stats.get('hit_rate', 0.0):.1%}")
        
        return {
            "before_optimization": stats,
            "optimization_result": optimization_result,
            "after_optimization": new_stats
        }
        
    except Exception as e:
        print(f"âŒ ç¼“å­˜ä¼˜åŒ–æ¼”ç¤ºå¤±è´¥: {e}")
        return None


def run_comprehensive_demo():
    """è¿è¡Œç»¼åˆæ¼”ç¤º"""
    print("ğŸš€ å¯åŠ¨è¯„ä¼°ç³»ç»Ÿç»¼åˆæ¼”ç¤º")
    print("=" * 80)
    
    results = {}
    
    # è¿è¡Œå„ä¸ªç¤ºä¾‹
    examples = [
        ("basic_evaluation", example_basic_evaluation),
        ("llm_intelligent", example_llm_intelligent_evaluation),
        ("multi_expert", example_multi_expert_evaluation),
        ("adversarial", example_adversarial_evaluation),
        ("phage_domain", example_phage_domain_evaluation),
        ("meta_cognitive", example_meta_cognitive_evaluation),
        ("supervision", example_supervision_system),
        ("cache_optimization", example_cache_optimization)
    ]
    
    for example_name, example_func in examples:
        try:
            print(f"\nğŸ”„ è¿è¡Œç¤ºä¾‹: {example_name}")
            result = example_func()
            results[example_name] = result
            
            if result:
                print(f"âœ… ç¤ºä¾‹ {example_name} å®Œæˆ")
            else:
                print(f"âš ï¸ ç¤ºä¾‹ {example_name} æœªè¿”å›ç»“æœ")
                
        except Exception as e:
            print(f"âŒ ç¤ºä¾‹ {example_name} æ‰§è¡Œå¤±è´¥: {e}")
            results[example_name] = None
        
        # æ·»åŠ åˆ†éš”ç¬¦
        print("-" * 40)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ¼”ç¤ºæ€»ç»“")
    print("=" * 80)
    
    successful = sum(1 for result in results.values() if result is not None)
    total = len(results)
    
    print(f"æ€»ç¤ºä¾‹æ•°: {total}")
    print(f"æˆåŠŸæ‰§è¡Œ: {successful}")
    print(f"æˆåŠŸç‡: {successful/total:.1%}")
    
    print(f"\nğŸ“Š å„ç¤ºä¾‹æ‰§è¡ŒçŠ¶æ€:")
    for example_name, result in results.items():
        status = "âœ… æˆåŠŸ" if result is not None else "âŒ å¤±è´¥"
        print(f"  {example_name}: {status}")
    
    return results


if __name__ == "__main__":
    """ä¸»ç¨‹åºå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    parser.add_argument("--example", type=str, choices=[
        "basic", "llm", "multi-expert", "adversarial", 
        "phage", "meta", "supervision", "cache", "all"
    ], default="all", help="é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹")
    
    args = parser.parse_args()
    
    # æ ¹æ®å‚æ•°è¿è¡Œç›¸åº”ç¤ºä¾‹
    if args.example == "basic":
        example_basic_evaluation()
    elif args.example == "llm":
        example_llm_intelligent_evaluation()
    elif args.example == "multi-expert":
        example_multi_expert_evaluation()
    elif args.example == "adversarial":
        example_adversarial_evaluation()
    elif args.example == "phage":
        example_phage_domain_evaluation()
    elif args.example == "meta":
        example_meta_cognitive_evaluation()
    elif args.example == "supervision":
        example_supervision_system()
    elif args.example == "cache":
        example_cache_optimization()
    elif args.example == "all":
        run_comprehensive_demo()
    else:
        print("è¯·é€‰æ‹©æœ‰æ•ˆçš„ç¤ºä¾‹ç±»å‹")
        parser.print_help()
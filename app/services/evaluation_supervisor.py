"""
Evaluation Quality Self-Supervision System

Monitors evaluation system performance, detects quality degradation,
and provides automatic calibration and supervision mechanisms.
"""

import json
import logging
import statistics
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from ..models import EvaluationConfig, EvaluationResult
from .evaluation_cache import get_evaluation_cache
from .meta_evaluator import get_meta_evaluator

logger = logging.getLogger(__name__)


@dataclass
class QualityMetric:
    """Quality metric for evaluation supervision"""

    name: str
    value: float
    threshold: float
    status: str  # "good", "warning", "critical"
    timestamp: datetime
    metadata: Dict[str, Any]


@dataclass
class SupervisionAlert:
    """Alert generated by supervision system"""

    alert_type: str
    severity: str  # "low", "medium", "high", "critical"
    message: str
    affected_components: List[str]
    recommended_actions: List[str]
    timestamp: datetime
    metadata: Dict[str, Any]


class EvaluationSupervisor:
    """Self-supervision system for evaluation quality monitoring"""

    def __init__(self, config: Optional[EvaluationConfig] = None):
        self.config = config or EvaluationConfig()
        self.meta_evaluator = get_meta_evaluator()
        self.evaluation_cache = get_evaluation_cache()

        # Quality metrics tracking
        self.quality_metrics = {}
        self.metric_history = defaultdict(lambda: deque(maxlen=100))
        self.alerts = deque(maxlen=50)

        # Performance monitoring
        self.performance_stats = {
            "evaluation_times": deque(maxlen=100),
            "success_rates": deque(maxlen=100),
            "cache_hit_rates": deque(maxlen=100),
            "error_counts": defaultdict(int),
            "last_reset": datetime.now(),
        }

        # Supervision thresholds
        self.thresholds = {
            "min_accuracy": 0.7,
            "min_consistency": 0.6,
            "max_bias_risk": 0.3,
            "min_cache_hit_rate": 0.3,
            "max_error_rate": 0.1,
            "max_evaluation_time": 30.0,  # seconds
            "min_confidence": 0.5,
        }

        # Auto-calibration settings
        self.auto_calibration_enabled = True
        self.calibration_history = deque(maxlen=20)

        self._lock = threading.RLock()
        self._last_supervision_check = datetime.now()

    def monitor_evaluation(
        self,
        evaluation_result: EvaluationResult,
        evaluation_method: str,
        execution_time: float,
        content: str,
        task_context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Monitor a single evaluation for quality and performance

        Args:
            evaluation_result: The evaluation result to monitor
            evaluation_method: Method used for evaluation
            execution_time: Time taken for evaluation
            content: Content that was evaluated
            task_context: Task context information

        Returns:
            Monitoring report with quality metrics and alerts
        """

        with self._lock:
            monitoring_report = {
                "timestamp": datetime.now().isoformat(),
                "evaluation_method": evaluation_method,
                "quality_metrics": {},
                "performance_metrics": {},
                "alerts": [],
                "recommendations": [],
            }

            try:
                # Update performance statistics
                self._update_performance_stats(execution_time, evaluation_result, evaluation_method)

                # Calculate quality metrics
                quality_metrics = self._calculate_quality_metrics(evaluation_result, evaluation_method, execution_time)
                monitoring_report["quality_metrics"] = quality_metrics

                # Update metric history
                for metric_name, metric in quality_metrics.items():
                    self.metric_history[metric_name].append(metric.value)
                    self.quality_metrics[metric_name] = metric

                # Check for quality issues
                alerts = self._check_quality_issues(quality_metrics)
                monitoring_report["alerts"] = [alert.__dict__ for alert in alerts]

                # Generate recommendations
                recommendations = self._generate_recommendations(quality_metrics, alerts)
                monitoring_report["recommendations"] = recommendations

                # Perform auto-calibration if needed
                if self.auto_calibration_enabled:
                    calibration_actions = self._perform_auto_calibration(quality_metrics)
                    if calibration_actions:
                        monitoring_report["calibration_actions"] = calibration_actions

                # Store alerts
                self.alerts.extend(alerts)

                logger.debug(
                    f"Evaluation monitoring completed: {len(alerts)} alerts, {len(recommendations)} recommendations"
                )

            except Exception as e:
                logger.error(f"Error during evaluation monitoring: {e}")
                monitoring_report["error"] = str(e)

            return monitoring_report

    def _update_performance_stats(
        self, execution_time: float, evaluation_result: EvaluationResult, evaluation_method: str
    ):
        """Update performance statistics"""

        # Record execution time
        self.performance_stats["evaluation_times"].append(execution_time)

        # Record success/failure
        success = evaluation_result.overall_score > 0.0
        self.performance_stats["success_rates"].append(1.0 if success else 0.0)

        # Update cache hit rate
        cache_stats = self.evaluation_cache.get_cache_stats()
        hit_rate = cache_stats.get("hit_rate", 0.0)
        self.performance_stats["cache_hit_rates"].append(hit_rate)

        # Reset error counts periodically
        if datetime.now() - self.performance_stats["last_reset"] > timedelta(hours=1):
            self.performance_stats["error_counts"].clear()
            self.performance_stats["last_reset"] = datetime.now()

    def _calculate_quality_metrics(
        self, evaluation_result: EvaluationResult, evaluation_method: str, execution_time: float
    ) -> Dict[str, QualityMetric]:
        """Calculate quality metrics for supervision"""

        metrics = {}
        now = datetime.now()

        # Accuracy metric (based on evaluation score)
        accuracy_value = evaluation_result.overall_score
        accuracy_status = (
            "good"
            if accuracy_value >= self.thresholds["min_accuracy"]
            else "warning" if accuracy_value >= 0.5 else "critical"
        )

        metrics["accuracy"] = QualityMetric(
            name="accuracy",
            value=accuracy_value,
            threshold=self.thresholds["min_accuracy"],
            status=accuracy_status,
            timestamp=now,
            metadata={"evaluation_method": evaluation_method},
        )

        # Consistency metric (based on recent evaluations)
        consistency_value = self._calculate_consistency_metric()
        consistency_status = (
            "good"
            if consistency_value >= self.thresholds["min_consistency"]
            else "warning" if consistency_value >= 0.4 else "critical"
        )

        metrics["consistency"] = QualityMetric(
            name="consistency",
            value=consistency_value,
            threshold=self.thresholds["min_consistency"],
            status=consistency_status,
            timestamp=now,
            metadata={"sample_size": len(self.metric_history["accuracy"])},
        )

        # Performance metric (based on execution time)
        performance_value = max(0.0, 1.0 - (execution_time / self.thresholds["max_evaluation_time"]))
        performance_status = (
            "good"
            if execution_time <= self.thresholds["max_evaluation_time"]
            else "warning" if execution_time <= self.thresholds["max_evaluation_time"] * 1.5 else "critical"
        )

        metrics["performance"] = QualityMetric(
            name="performance",
            value=performance_value,
            threshold=0.7,
            status=performance_status,
            timestamp=now,
            metadata={"execution_time": execution_time},
        )

        # Cache efficiency metric
        cache_stats = self.evaluation_cache.get_cache_stats()
        cache_hit_rate = cache_stats.get("hit_rate", 0.0)
        cache_status = (
            "good"
            if cache_hit_rate >= self.thresholds["min_cache_hit_rate"]
            else "warning" if cache_hit_rate >= 0.2 else "critical"
        )

        metrics["cache_efficiency"] = QualityMetric(
            name="cache_efficiency",
            value=cache_hit_rate,
            threshold=self.thresholds["min_cache_hit_rate"],
            status=cache_status,
            timestamp=now,
            metadata=cache_stats,
        )

        # Confidence metric (based on evaluation metadata)
        confidence_value = (
            evaluation_result.metadata.get("confidence_level", 0.5) if evaluation_result.metadata else 0.5
        )
        confidence_status = (
            "good"
            if confidence_value >= self.thresholds["min_confidence"]
            else "warning" if confidence_value >= 0.3 else "critical"
        )

        metrics["confidence"] = QualityMetric(
            name="confidence",
            value=confidence_value,
            threshold=self.thresholds["min_confidence"],
            status=confidence_status,
            timestamp=now,
            metadata={"evaluation_method": evaluation_method},
        )

        return metrics

    def _calculate_consistency_metric(self) -> float:
        """Calculate consistency metric based on recent evaluations"""

        accuracy_history = list(self.metric_history["accuracy"])
        if len(accuracy_history) < 3:
            return 1.0  # Not enough data, assume consistent

        # Calculate coefficient of variation (lower is more consistent)
        mean_accuracy = statistics.mean(accuracy_history)
        if mean_accuracy == 0:
            return 0.0

        std_accuracy = statistics.stdev(accuracy_history)
        cv = std_accuracy / mean_accuracy

        # Convert to consistency score (higher is better)
        consistency = max(0.0, 1.0 - cv)
        return consistency

    def _check_quality_issues(self, quality_metrics: Dict[str, QualityMetric]) -> List[SupervisionAlert]:
        """Check for quality issues and generate alerts"""

        alerts = []
        now = datetime.now()

        for metric_name, metric in quality_metrics.items():
            if metric.status == "critical":
                alerts.append(
                    SupervisionAlert(
                        alert_type="quality_degradation",
                        severity="critical",
                        message=f"Critical quality issue detected in {metric_name}: {metric.value:.3f} < {metric.threshold:.3f}",
                        affected_components=[metric_name],
                        recommended_actions=[
                            f"Investigate {metric_name} degradation",
                            "Review recent evaluation results",
                            "Consider recalibrating evaluation parameters",
                        ],
                        timestamp=now,
                        metadata=metric.metadata,
                    )
                )

            elif metric.status == "warning":
                alerts.append(
                    SupervisionAlert(
                        alert_type="quality_warning",
                        severity="medium",
                        message=f"Quality warning for {metric_name}: {metric.value:.3f} approaching threshold {metric.threshold:.3f}",
                        affected_components=[metric_name],
                        recommended_actions=[f"Monitor {metric_name} closely", "Consider preventive measures"],
                        timestamp=now,
                        metadata=metric.metadata,
                    )
                )

        # Check for performance degradation trends
        if len(self.performance_stats["evaluation_times"]) >= 10:
            recent_times = list(self.performance_stats["evaluation_times"])[-10:]
            avg_recent_time = statistics.mean(recent_times)

            if avg_recent_time > self.thresholds["max_evaluation_time"] * 1.2:
                alerts.append(
                    SupervisionAlert(
                        alert_type="performance_degradation",
                        severity="high",
                        message=f"Performance degradation detected: average evaluation time {avg_recent_time:.2f}s",
                        affected_components=["performance"],
                        recommended_actions=[
                            "Optimize evaluation algorithms",
                            "Check system resources",
                            "Clear evaluation cache if needed",
                        ],
                        timestamp=now,
                        metadata={"avg_time": avg_recent_time, "threshold": self.thresholds["max_evaluation_time"]},
                    )
                )

        # Check for error rate increases
        total_requests = sum(self.performance_stats["error_counts"].values())
        if total_requests > 0:
            error_rate = total_requests / (len(self.performance_stats["success_rates"]) + total_requests)
            if error_rate > self.thresholds["max_error_rate"]:
                alerts.append(
                    SupervisionAlert(
                        alert_type="error_rate_increase",
                        severity="high",
                        message=f"High error rate detected: {error_rate:.3f} > {self.thresholds['max_error_rate']:.3f}",
                        affected_components=["reliability"],
                        recommended_actions=[
                            "Investigate error causes",
                            "Review system logs",
                            "Check external dependencies",
                        ],
                        timestamp=now,
                        metadata={
                            "error_rate": error_rate,
                            "error_counts": dict(self.performance_stats["error_counts"]),
                        },
                    )
                )

        return alerts

    def _generate_recommendations(
        self, quality_metrics: Dict[str, QualityMetric], alerts: List[SupervisionAlert]
    ) -> List[str]:
        """Generate recommendations based on quality metrics and alerts"""

        recommendations = []

        # Accuracy recommendations
        accuracy_metric = quality_metrics.get("accuracy")
        if accuracy_metric and accuracy_metric.value < 0.7:
            recommendations.append("Consider using more sophisticated evaluation methods")
            recommendations.append("Review evaluation criteria and thresholds")

        # Consistency recommendations
        consistency_metric = quality_metrics.get("consistency")
        if consistency_metric and consistency_metric.value < 0.6:
            recommendations.append("Investigate evaluation inconsistencies")
            recommendations.append("Consider implementing evaluation result smoothing")

        # Performance recommendations
        performance_metric = quality_metrics.get("performance")
        if performance_metric and performance_metric.status in ["warning", "critical"]:
            recommendations.append("Optimize evaluation algorithms for better performance")
            recommendations.append("Consider increasing cache size or TTL")

        # Cache efficiency recommendations
        cache_metric = quality_metrics.get("cache_efficiency")
        if cache_metric and cache_metric.value < 0.3:
            recommendations.append("Review cache configuration and policies")
            recommendations.append("Consider warming up cache with common evaluations")

        # Alert-based recommendations
        critical_alerts = [alert for alert in alerts if alert.severity == "critical"]
        if critical_alerts:
            recommendations.append("Address critical quality issues immediately")
            recommendations.append("Consider temporarily disabling affected evaluation methods")

        return recommendations[:10]  # Limit to top 10 recommendations

    def _perform_auto_calibration(self, quality_metrics: Dict[str, QualityMetric]) -> List[str]:
        """Perform automatic calibration based on quality metrics"""

        calibration_actions = []

        # Auto-adjust thresholds based on performance
        accuracy_metric = quality_metrics.get("accuracy")
        if accuracy_metric:
            accuracy_history = list(self.metric_history["accuracy"])
            if len(accuracy_history) >= 20:
                # Calculate adaptive threshold
                recent_avg = statistics.mean(accuracy_history[-10:])
                historical_avg = statistics.mean(accuracy_history[:-10])

                if recent_avg < historical_avg * 0.9:  # 10% degradation
                    # Lower threshold temporarily
                    new_threshold = max(0.5, recent_avg * 0.9)
                    if new_threshold != self.thresholds["min_accuracy"]:
                        self.thresholds["min_accuracy"] = new_threshold
                        calibration_actions.append(f"Auto-adjusted accuracy threshold to {new_threshold:.3f}")

                        self.calibration_history.append(
                            {
                                "timestamp": datetime.now().isoformat(),
                                "action": "threshold_adjustment",
                                "metric": "accuracy",
                                "old_value": self.thresholds["min_accuracy"],
                                "new_value": new_threshold,
                                "reason": "performance_degradation",
                            }
                        )

        # Auto-optimize cache settings
        cache_metric = quality_metrics.get("cache_efficiency")
        if cache_metric and cache_metric.value < 0.2:
            # Trigger cache optimization
            try:
                optimization_result = self.evaluation_cache.optimize_cache()
                if optimization_result.get("entries_removed", 0) > 0:
                    calibration_actions.append(
                        f"Auto-optimized cache: removed {optimization_result['entries_removed']} entries"
                    )
            except Exception as e:
                logger.error(f"Auto-cache optimization failed: {e}")

        return calibration_actions

    def get_supervision_report(self) -> Dict[str, Any]:
        """Generate comprehensive supervision report"""

        with self._lock:
            now = datetime.now()

            # Calculate overall system health
            health_score = self._calculate_system_health()

            # Get recent alerts
            recent_alerts = [alert for alert in self.alerts if now - alert.timestamp <= timedelta(hours=24)]

            # Performance summary
            performance_summary = {}
            if self.performance_stats["evaluation_times"]:
                performance_summary["avg_evaluation_time"] = statistics.mean(self.performance_stats["evaluation_times"])
                performance_summary["max_evaluation_time"] = max(self.performance_stats["evaluation_times"])

            if self.performance_stats["success_rates"]:
                performance_summary["success_rate"] = statistics.mean(self.performance_stats["success_rates"])

            if self.performance_stats["cache_hit_rates"]:
                performance_summary["avg_cache_hit_rate"] = statistics.mean(self.performance_stats["cache_hit_rates"])

            # Quality trends
            quality_trends = {}
            for metric_name, history in self.metric_history.items():
                if len(history) >= 5:
                    recent_avg = statistics.mean(list(history)[-5:])
                    historical_avg = statistics.mean(list(history)[:-5]) if len(history) > 5 else recent_avg
                    trend = (
                        "improving"
                        if recent_avg > historical_avg
                        else "declining" if recent_avg < historical_avg else "stable"
                    )
                    quality_trends[metric_name] = {
                        "trend": trend,
                        "recent_avg": recent_avg,
                        "historical_avg": historical_avg,
                    }

            return {
                "timestamp": now.isoformat(),
                "system_health": {
                    "overall_score": health_score,
                    "status": "healthy" if health_score >= 0.8 else "degraded" if health_score >= 0.6 else "critical",
                },
                "current_metrics": {
                    name: {"value": metric.value, "status": metric.status, "threshold": metric.threshold}
                    for name, metric in self.quality_metrics.items()
                },
                "performance_summary": performance_summary,
                "quality_trends": quality_trends,
                "recent_alerts": [alert.__dict__ for alert in recent_alerts],
                "alert_summary": {
                    "total": len(recent_alerts),
                    "critical": len([a for a in recent_alerts if a.severity == "critical"]),
                    "high": len([a for a in recent_alerts if a.severity == "high"]),
                    "medium": len([a for a in recent_alerts if a.severity == "medium"]),
                },
                "calibration_history": list(self.calibration_history),
                "supervision_config": {
                    "thresholds": self.thresholds.copy(),
                    "auto_calibration_enabled": self.auto_calibration_enabled,
                },
            }

    def _calculate_system_health(self) -> float:
        """Calculate overall system health score"""

        if not self.quality_metrics:
            return 0.5  # Unknown health

        # Weight different metrics
        weights = {
            "accuracy": 0.3,
            "consistency": 0.2,
            "performance": 0.2,
            "cache_efficiency": 0.15,
            "confidence": 0.15,
        }

        weighted_score = 0.0
        total_weight = 0.0

        for metric_name, weight in weights.items():
            if metric_name in self.quality_metrics:
                metric = self.quality_metrics[metric_name]
                # Convert status to score
                status_scores = {"good": 1.0, "warning": 0.6, "critical": 0.2}
                status_score = status_scores.get(metric.status, 0.5)

                # Combine with actual value
                combined_score = (metric.value + status_score) / 2
                weighted_score += combined_score * weight
                total_weight += weight

        if total_weight == 0:
            return 0.5

        return weighted_score / total_weight

    def update_thresholds(self, new_thresholds: Dict[str, float]) -> bool:
        """Update supervision thresholds"""

        try:
            with self._lock:
                for key, value in new_thresholds.items():
                    if key in self.thresholds:
                        old_value = self.thresholds[key]
                        self.thresholds[key] = value

                        self.calibration_history.append(
                            {
                                "timestamp": datetime.now().isoformat(),
                                "action": "manual_threshold_update",
                                "metric": key,
                                "old_value": old_value,
                                "new_value": value,
                                "reason": "manual_adjustment",
                            }
                        )

                logger.info(f"Updated supervision thresholds: {new_thresholds}")
                return True

        except Exception as e:
            logger.error(f"Error updating thresholds: {e}")
            return False

    def reset_supervision_state(self) -> bool:
        """Reset supervision state (for testing or recovery)"""

        try:
            with self._lock:
                self.quality_metrics.clear()
                self.metric_history.clear()
                self.alerts.clear()
                self.performance_stats = {
                    "evaluation_times": deque(maxlen=100),
                    "success_rates": deque(maxlen=100),
                    "cache_hit_rates": deque(maxlen=100),
                    "error_counts": defaultdict(int),
                    "last_reset": datetime.now(),
                }
                self.calibration_history.clear()

                logger.info("Supervision state reset successfully")
                return True

        except Exception as e:
            logger.error(f"Error resetting supervision state: {e}")
            return False


# Global supervisor instance
_evaluation_supervisor = None
_supervisor_lock = threading.Lock()


def get_evaluation_supervisor() -> EvaluationSupervisor:
    """Get global evaluation supervisor instance"""
    global _evaluation_supervisor

    if _evaluation_supervisor is None:
        with _supervisor_lock:
            if _evaluation_supervisor is None:
                _evaluation_supervisor = EvaluationSupervisor()

    return _evaluation_supervisor


def monitor_evaluation(
    evaluation_result: EvaluationResult,
    evaluation_method: str,
    execution_time: float,
    content: str,
    task_context: Dict[str, Any],
) -> Dict[str, Any]:
    """Monitor evaluation with supervision system"""
    supervisor = get_evaluation_supervisor()
    return supervisor.monitor_evaluation(evaluation_result, evaluation_method, execution_time, content, task_context)


def get_supervision_report() -> Dict[str, Any]:
    """Get supervision system report"""
    supervisor = get_evaluation_supervisor()
    return supervisor.get_supervision_report()

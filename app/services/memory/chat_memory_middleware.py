"""
Chat Memory Middleware - èŠå¤©è®°å¿†ä¸­é—´ä»¶

ä½¿ç”¨LLMæ™ºèƒ½åˆ†æå’Œä¿å­˜é‡è¦çš„èŠå¤©æ¶ˆæ¯ä¸ºè®°å¿†
"""

import json
import logging
from typing import Optional

from ...llm import get_default_client
from ...models_memory import ImportanceLevel, MemoryType
from .memory_hooks import get_memory_hooks

logger = logging.getLogger(__name__)


class ChatMemoryMiddleware:
    """èŠå¤©è®°å¿†ä¸­é—´ä»¶ - ä½¿ç”¨LLMæ™ºèƒ½åˆ¤æ–­å’Œä¿å­˜é‡è¦å¯¹è¯"""

    def __init__(self):
        self.hooks = get_memory_hooks()
        self.llm_client = get_default_client()
        self.enabled = True

    async def process_message(
        self,
        content: str,
        role: str = "user",
        session_id: Optional[str] = None,
        force_save: bool = False,
    ) -> Optional[str]:
        """
        å¤„ç†èŠå¤©æ¶ˆæ¯ï¼Œä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦ä¿å­˜ä¸ºè®°å¿†
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
            role: è§’è‰² (user/assistant)
            session_id: ä¼šè¯ID
            force_save: å¼ºåˆ¶ä¿å­˜
            
        Returns:
            è®°å¿†IDï¼Œå¦‚æœæœªä¿å­˜åˆ™è¿”å›None
        """
        if not self.enabled and not force_save:
            return None
        
        # ä½¿ç”¨LLMåˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜
        should_save, importance, memory_type = await self._should_save_message(
            content, role, force_save
        )
        
        if not should_save:
            return None
        
        # ä¿å­˜ä¸ºè®°å¿†ï¼ˆä½¿ç”¨LLMåˆ¤æ–­çš„ç±»å‹ï¼‰
        try:
            from ...models_memory import SaveMemoryRequest
            from .memory_service import get_memory_service
            
            memory_service = get_memory_service()
            
            # æ·»åŠ è§’è‰²æ ‡è¯†
            memory_content = f"[{role}] {content}"
            
            tags = ["å¯¹è¯", role]
            if session_id:
                tags.append(f"session:{session_id}")
            
            request = SaveMemoryRequest(
                content=memory_content,
                memory_type=memory_type,
                importance=importance,
                tags=tags,
            )
            
            response = await memory_service.save_memory(request)
            memory_id = response.memory_id
            
            if memory_id:
                logger.info(f"ğŸ’¾ èŠå¤©æ¶ˆæ¯å·²ä¿å­˜ä¸ºè®°å¿† ({memory_type.value}/{importance.value}): {memory_id[:8]}...")
            
            return memory_id
            
        except Exception as e:
            logger.error(f"ä¿å­˜èŠå¤©è®°å¿†å¤±è´¥: {e}")
            return None

    async def _should_save_message(
        self,
        content: str,
        role: str,
        force_save: bool = False,
    ) -> tuple[bool, ImportanceLevel, Optional[MemoryType]]:
        """
        ä½¿ç”¨LLMåˆ¤æ–­æ¶ˆæ¯æ˜¯å¦åº”è¯¥ä¿å­˜ä»¥åŠé‡è¦æ€§çº§åˆ«
        
        Returns:
            (æ˜¯å¦ä¿å­˜, é‡è¦æ€§çº§åˆ«, è®°å¿†ç±»å‹)
        """
        if force_save:
            return True, ImportanceLevel.HIGH, MemoryType.CONVERSATION
        
        # å¤ªçŸ­çš„æ¶ˆæ¯ç›´æ¥è·³è¿‡
        if len(content) < 10:
            return False, ImportanceLevel.LOW, None
        
        # ä½¿ç”¨LLMåˆ¤æ–­ï¼Œæœ€å¤šé‡è¯•3æ¬¡
        max_retries = 3
        last_error = None
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logger.info(f"ğŸ”„ LLMåˆ¤æ–­é‡è¯• {attempt + 1}/{max_retries}")
                
                prompt = f"""You are an intelligent memory system analyzer. Analyze the following conversation message and determine if it's worth saving as long-term memory.

Role: {role}
Message Content: {content}

Analyze from the following dimensions:
1. Does it contain important knowledge, experience, or insights?
2. Is it a critical question, error, or solution?
3. Does it have reference value for future conversations or tasks?
4. Does it contain configuration, settings, or important decisions?

Return your judgment in JSON format:
{{
    "should_save": true/false,  // Whether to save
    "importance": "low/medium/high/critical",  // Importance level
    "memory_type": "knowledge/experience/conversation/context",  // Memory type
    "reason": "Brief explanation"
}}

Only return JSON, no other content."""

                # æ³¨æ„ï¼šllm_client.chat() æ˜¯åŒæ­¥æ–¹æ³•ï¼Œéœ€è¦ç”¨ run_in_executor
                import asyncio
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: self.llm_client.chat(prompt, temperature=0.3)
                )
                
                # è§£æLLMå“åº”
                response_text = response.strip() if isinstance(response, str) else response.get("content", "").strip()
                
                # å°è¯•æå–JSON
                if "```json" in response_text:
                    response_text = response_text.split("```json")[1].split("```")[0].strip()
                elif "```" in response_text:
                    response_text = response_text.split("```")[1].split("```")[0].strip()
                
                result = json.loads(response_text)
                
                should_save = result.get("should_save", False)
                importance_str = result.get("importance", "low").lower()
                memory_type_str = result.get("memory_type", "conversation").lower()
                reason = result.get("reason", "")
                
                # è½¬æ¢ä¸ºæšä¸¾
                importance_map = {
                    "low": ImportanceLevel.LOW,
                    "medium": ImportanceLevel.MEDIUM,
                    "high": ImportanceLevel.HIGH,
                    "critical": ImportanceLevel.CRITICAL,
                }
                importance = importance_map.get(importance_str, ImportanceLevel.MEDIUM)
                
                memory_type_map = {
                    "knowledge": MemoryType.KNOWLEDGE,
                    "experience": MemoryType.EXPERIENCE,
                    "conversation": MemoryType.CONVERSATION,
                    "context": MemoryType.CONTEXT,
                }
                memory_type = memory_type_map.get(memory_type_str, MemoryType.CONVERSATION)
                
                if should_save:
                    logger.info(f"ğŸ¤– LLMåˆ¤æ–­åº”ä¿å­˜: {importance_str} - {reason}")
                else:
                    logger.debug(f"ğŸ¤– LLMåˆ¤æ–­ä¸ä¿å­˜: {reason}")
                
                # æˆåŠŸï¼Œç›´æ¥è¿”å›
                return should_save, importance, memory_type
                
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸  LLMåˆ¤æ–­å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­ä¸‹ä¸€æ¬¡
                if attempt < max_retries - 1:
                    continue
                # å¦åˆ™è·³å‡ºå¾ªç¯
                break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"âŒ LLMåˆ¤æ–­å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {last_error}")
        return False, ImportanceLevel.LOW, None

    async def process_assistant_response(
        self,
        content: str,
        session_id: Optional[str] = None,
    ) -> Optional[str]:
        """
        å¤„ç†åŠ©æ‰‹å“åº”ï¼Œä½¿ç”¨LLMåˆ¤æ–­
        
        Args:
            content: å“åº”å†…å®¹
            session_id: ä¼šè¯ID
            
        Returns:
            è®°å¿†ID
        """
        return await self.process_message(
            content=content,
            role="assistant",
            session_id=session_id,
            force_save=False
        )

    def enable(self):
        """å¯ç”¨ä¸­é—´ä»¶"""
        self.enabled = True
        logger.info("âœ… Chat memory middleware enabled")

    def disable(self):
        """ç¦ç”¨ä¸­é—´ä»¶"""
        self.enabled = False
        logger.info("â¸ï¸  Chat memory middleware disabled")


# å…¨å±€å•ä¾‹
_chat_memory_middleware: Optional[ChatMemoryMiddleware] = None


def get_chat_memory_middleware() -> ChatMemoryMiddleware:
    """è·å–èŠå¤©è®°å¿†ä¸­é—´ä»¶å®ä¾‹"""
    global _chat_memory_middleware
    if _chat_memory_middleware is None:
        _chat_memory_middleware = ChatMemoryMiddleware()
    return _chat_memory_middleware

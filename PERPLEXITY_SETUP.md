# Perplexity API é…ç½®æŒ‡å—

## ğŸ¯ **å¿«é€Ÿé…ç½®**

### 1. åˆ›å»º `.env` æ–‡ä»¶
åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# LLM Provider Configuration  
LLM_PROVIDER=perplexity

# Perplexity API Configuration
PERPLEXITY_API_KEY=your_actual_perplexity_api_key_here
PERPLEXITY_API_URL=https://api.perplexity.ai/chat/completions
PERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online

# General Settings
LLM_MOCK=false
LLM_RETRIES=3
DATABASE_URL=sqlite:///./tasks.db
BASE_URL=http://127.0.0.1:8000
```

### 2. è·å–Perplexity APIå¯†é’¥
1. è®¿é—® [Perplexity API Settings](https://www.perplexity.ai/settings/api)
2. ç™»å½•æˆ–åˆ›å»ºè´¦æˆ·
3. ç”Ÿæˆæ–°çš„APIå¯†é’¥
4. å¤åˆ¶å¯†é’¥å¹¶æ›¿æ¢ä¸Šé¢çš„ `your_actual_perplexity_api_key_here`

### 3. å¯ç”¨æ¨¡å‹
- `llama-3.1-sonar-small-128k-online` (æ¨èï¼Œè”ç½‘æœç´¢)
- `llama-3.1-sonar-large-128k-online` (æ›´å¼ºæ€§èƒ½)
- `llama-3.1-sonar-huge-128k-online` (æœ€å¼ºæ€§èƒ½)
- `llama-3.1-8b-instruct` (å¿«é€Ÿç¦»çº¿)
- `llama-3.1-70b-instruct` (å¹³è¡¡ç¦»çº¿)

### 4. æµ‹è¯•é…ç½®
```bash
# å¯åŠ¨åç«¯
conda activate LLM
python -m uvicorn app.main:app --host 127.0.0.1 --port 8000 --reload

# æ–°ç»ˆç«¯å¯åŠ¨èŠå¤©
conda activate LLM
python -m cli.main --chat
```

## ğŸ’¡ **åˆ‡æ¢å›GLM**
åªéœ€ä¿®æ”¹ `.env` æ–‡ä»¶ï¼š
```bash
LLM_PROVIDER=glm
GLM_API_KEY=your_glm_api_key
```

## ğŸ”§ **ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§**
1. å‘½ä»¤è¡Œå‚æ•° (`--provider perplexity`)
2. ç¯å¢ƒå˜é‡ (`LLM_PROVIDER=perplexity`)
3. .envæ–‡ä»¶è®¾ç½®
4. ç³»ç»Ÿé»˜è®¤ (glm)

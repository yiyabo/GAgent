# é«˜çº§è¯„ä¼°ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬ç³»ç»Ÿæä¾›äº†ä¸€å¥—å®Œæ•´çš„å†…å®¹è¯„ä¼°å’Œè´¨é‡ç®¡ç†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«å¤šç§å…ˆè¿›çš„è¯„ä¼°æ¨¡å¼å’Œè‡ªç›‘ç£æœºåˆ¶ã€‚ç³»ç»Ÿæ”¯æŒä»åŸºç¡€è¯„ä¼°åˆ°å¤æ‚çš„å¤šä¸“å®¶åä½œè¯„ä¼°ï¼Œé€‚ç”¨äºå„ç§å†…å®¹ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶åœºæ™¯ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ§  æ™ºèƒ½è¯„ä¼°æ¨¡å¼

- **LLMæ™ºèƒ½è¯„ä¼°**: åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ·±åº¦è¯­ä¹‰ç†è§£è¯„ä¼°
- **å¤šä¸“å®¶è¯„ä¼°**: 5ä½ä¸“ä¸šé¢†åŸŸä¸“å®¶çš„åä½œè¯„ä¼°ç³»ç»Ÿ
- **å¯¹æŠ—æ€§è¯„ä¼°**: ç”Ÿæˆå™¨ä¸æ‰¹è¯„è€…çš„å¯¹æŠ—æ€§æ”¹è¿›æœºåˆ¶
- **å…ƒè®¤çŸ¥è¯„ä¼°**: è¯„ä¼°è´¨é‡çš„è‡ªæˆ‘åæ€å’Œè®¤çŸ¥åè§æ£€æµ‹
- **é¢†åŸŸä¸“ä¸šè¯„ä¼°**: é’ˆå¯¹å™¬èŒä½“ç ”ç©¶çš„ä¸“ä¸šæœ¯è¯­å’Œä¸´åºŠç›¸å…³æ€§è¯„ä¼°

### âš¡ æ€§èƒ½ä¼˜åŒ–

- **å¤šå±‚ç¼“å­˜ç³»ç»Ÿ**: å†…å­˜ç¼“å­˜ + SQLiteæŒä¹…åŒ–ç¼“å­˜
- **æ™ºèƒ½ç¼“å­˜ç­–ç•¥**: åŸºäºä½¿ç”¨é¢‘ç‡å’Œæ—¶é—´çš„è‡ªåŠ¨æ¸…ç†
- **æ€§èƒ½ç›‘æ§**: å®æ—¶æ€§èƒ½ç»Ÿè®¡å’Œä¼˜åŒ–å»ºè®®

### ğŸ” è´¨é‡ç›‘ç£

- **è‡ªç›‘ç£æœºåˆ¶**: è‡ªåŠ¨æ£€æµ‹è¯„ä¼°ç³»ç»Ÿè´¨é‡ä¸‹é™
- **å®æ—¶ç›‘æ§**: å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€æ€§èƒ½ç­‰å¤šç»´åº¦ç›‘æ§
- **è‡ªåŠ¨æ ¡å‡†**: åŸºäºå†å²æ•°æ®çš„é˜ˆå€¼è‡ªåŠ¨è°ƒæ•´
- **è­¦æŠ¥ç³»ç»Ÿ**: å¤šçº§åˆ«è­¦æŠ¥å’Œæ¨èæªæ–½

## å¿«é€Ÿå¼€å§‹

è¯„ä¼°ç›¸å…³ CLI/REST çš„å¸¸ç”¨å‘½ä»¤ä¸ç«¯åˆ°ç«¯ç¤ºä¾‹å·²æ”¶æ•›åˆ° `docs/QUICK_START.md`ï¼Œæœ¬æŒ‡å—èšç„¦äºé…ç½®ã€ä»£ç ç¤ºä¾‹ä¸æœ€ä½³å®è·µã€‚

## è¯¦ç»†åŠŸèƒ½è¯´æ˜

### LLMæ™ºèƒ½è¯„ä¼°å™¨

LLMæ™ºèƒ½è¯„ä¼°å™¨ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰ç†è§£ï¼Œæä¾›6ä¸ªç»´åº¦çš„è¯„ä¼°ï¼š

- **ç›¸å…³æ€§ (Relevance)**: å†…å®¹ä¸ä»»åŠ¡è¦æ±‚çš„åŒ¹é…åº¦
- **å®Œæ•´æ€§ (Completeness)**: å†…å®¹çš„å…¨é¢æ€§å’Œå®Œæ•´æ€§
- **å‡†ç¡®æ€§ (Accuracy)**: ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§
- **æ¸…æ™°åº¦ (Clarity)**: è¡¨è¾¾çš„æ¸…æ™°åº¦å’Œå¯ç†è§£æ€§
- **è¿è´¯æ€§ (Coherence)**: é€»è¾‘ç»“æ„å’Œå†…å®¹è¿è´¯æ€§
- **ç§‘å­¦ä¸¥è°¨æ€§ (Scientific Rigor)**: ç§‘å­¦æ–¹æ³•å’Œè¯æ®çš„ä¸¥è°¨æ€§

#### ä½¿ç”¨ç¤ºä¾‹

```python
from app.execution.executors.enhanced import execute_task_with_llm_evaluation
from app.models import EvaluationConfig

# é…ç½®è¯„ä¼°å‚æ•°
config = EvaluationConfig(
    quality_threshold=0.8,
    max_iterations=3,
    strict_mode=True,
    evaluation_dimensions=["relevance", "completeness", "accuracy", "clarity", "coherence", "scientific_rigor"]
)

# æ‰§è¡ŒLLMæ™ºèƒ½è¯„ä¼°
result = execute_task_with_llm_evaluation(
    task=task,
    evaluation_config=config,
    use_context=True
)

print(f"æœ€ç»ˆè¯„åˆ†: {result.evaluation.overall_score:.3f}")
print(f"æ‰§è¡ŒçŠ¶æ€: {result.status}")
print(f"è¿­ä»£æ¬¡æ•°: {result.iterations_completed}")
```

## ä¸Šä¸‹æ–‡ç­–ç•¥ä¸é¢„ç®—æ§åˆ¶

è¯„ä¼°ä¸æ‰§è¡Œæ—¶å¯é€šè¿‡ä¸Šä¸‹æ–‡å’Œé¢„ç®—å‚æ•°ç²¾ç»†æ§åˆ¶æç¤ºè¯ä¸Šä¸‹æ–‡ï¼Œå…¼é¡¾è´¨é‡ä¸æˆæœ¬ï¼š

- ä¸Šä¸‹æ–‡æ”¶é›†ï¼š
  - **include_deps**: æ˜¯å¦åŒ…å«ä¾èµ–ä»»åŠ¡è¾“å‡ºï¼ˆé»˜è®¤ trueï¼‰
  - **include_plan**: æ˜¯å¦åŒ…å«åŒè®¡åˆ’å…„å¼Ÿä»»åŠ¡ï¼ˆé»˜è®¤ trueï¼‰
  - **include_ancestors / include_siblings**: æ˜¯å¦åŒ…å«ç¥–å…ˆ/åŒçº§ï¼ˆé»˜è®¤ falseï¼‰
  - **semantic_k / min_similarity**: GLM è¯­ä¹‰æ£€ç´¢æ•°é‡ä¸ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 5 / 0.1ï¼‰
  - **hierarchy_k**: å±‚æ¬¡æ£€ç´¢æ•°é‡ï¼ˆé»˜è®¤ 3ï¼‰

- é¢„ç®—è£å‰ªï¼š
  - **max_chars**: åˆå¹¶ä¸Šä¸‹æ–‡çš„æ€»å­—ç¬¦é¢„ç®—ï¼ˆNone è¡¨ç¤ºä¸è£å‰ªï¼‰
  - **per_section_max**: æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆNone è¡¨ç¤ºä¸é™åˆ¶ï¼‰
  - **strategy**: `truncate` æˆ– `sentence`ï¼ˆåœ¨æœ‰é¢„ç®—å‚æ•°æ—¶ç”Ÿæ•ˆï¼‰

è¯·æ±‚ç¤ºä¾‹ï¼ˆREST /tasks/{id}/context/previewï¼‰ï¼š

```json
{
  "include_deps": true,
  "include_plan": true,
  "semantic_k": 5,
  "min_similarity": 0.15,
  "include_ancestors": false,
  "include_siblings": false,
  "hierarchy_k": 3,
  "max_chars": 6000,
  "per_section_max": 1200,
  "strategy": "truncate"
}
```

ä¸¥æ ¼è¯„ä¼°å»ºè®®ï¼šå°† `quality_threshold` â‰¥ 0.92ï¼Œ`max_iterations` è®¾ä¸º 3-5ï¼Œç»´åº¦æƒé‡ä¾§é‡ **accuracy** ä¸ **scientific_rigor**ï¼Œå¯æœ‰æ•ˆæ‹‰å¼€ä¸åŒé…ç½®çš„è¯„åˆ†å·®å¼‚ã€‚

### å¤šä¸“å®¶è¯„ä¼°ç³»ç»Ÿ

å¤šä¸“å®¶è¯„ä¼°ç³»ç»Ÿæ¨¡æ‹Ÿ5ä½ä¸åŒé¢†åŸŸçš„ä¸“å®¶è¿›è¡Œåä½œè¯„ä¼°ï¼š

1. **ç†è®ºç”Ÿç‰©å­¦å®¶**: å…³æ³¨ç§‘å­¦ç†è®ºå’Œç”Ÿç‰©å­¦åŸç†
2. **ä¸´åºŠåŒ»å¸ˆ**: é‡è§†ä¸´åºŠåº”ç”¨å’Œæ‚£è€…å®‰å…¨
3. **ç›‘ç®¡ä¸“å®¶**: ä¸“æ³¨æ³•è§„åˆè§„å’Œå®‰å…¨æ ‡å‡†
4. **ç ”ç©¶ç§‘å­¦å®¶**: å¼ºè°ƒç ”ç©¶æ–¹æ³•å’Œå®éªŒè®¾è®¡
5. **ç”Ÿç‰©æŠ€æœ¯ä¼ä¸šå®¶**: è€ƒè™‘å•†ä¸šå¯è¡Œæ€§å’Œå¸‚åœºåº”ç”¨

#### ä¸“å®¶è¯„ä¼°æµç¨‹

```python
from app.services.evaluation.expert_evaluator import get_multi_expert_evaluator

evaluator = get_multi_expert_evaluator()

# å¤šä¸“å®¶è¯„ä¼°
result = evaluator.evaluate_with_multiple_experts(
    content="å¾…è¯„ä¼°å†…å®¹",
    task_context={"name": "ä»»åŠ¡åç§°", "task_type": "content_generation"},
    selected_experts=["theoretical_biologist", "clinical_physician"],  # å¯é€‰ï¼šæŒ‡å®šä¸“å®¶
    iteration=1
)

# æŸ¥çœ‹ä¸“å®¶å…±è¯†
consensus = result["consensus"]
print(f"ä¸“å®¶å…±è¯†è¯„åˆ†: {consensus['overall_score']:.3f}")
print(f"å…±è¯†ç½®ä¿¡åº¦: {consensus['consensus_confidence']:.3f}")

# æŸ¥çœ‹ä¸“å®¶åˆ†æ­§
disagreements = result["disagreements"]
for disagreement in disagreements:
    print(f"åˆ†æ­§é¢†åŸŸ: {disagreement['field']}")
    print(f"åˆ†æ­§ç¨‹åº¦: {disagreement['disagreement_level']:.2f}")
```

### å¯¹æŠ—æ€§è¯„ä¼°æœºåˆ¶

å¯¹æŠ—æ€§è¯„ä¼°é‡‡ç”¨ç”Ÿæˆå™¨ä¸æ‰¹è¯„è€…çš„å¯¹æŠ—æ¨¡å¼ï¼Œé€šè¿‡å¤šè½®æ”¹è¿›æå‡å†…å®¹è´¨é‡ï¼š

#### å¯¹æŠ—æ€§è¯„ä¼°ç»„ä»¶

1. **å†…å®¹ç”Ÿæˆå™¨ (ContentGenerator)**: è´Ÿè´£ç”Ÿæˆå’Œæ”¹è¿›å†…å®¹
2. **å†…å®¹æ‰¹è¯„è€… (ContentCritic)**: è´Ÿè´£å‘ç°é—®é¢˜å’Œæä¾›æ”¹è¿›å»ºè®®

#### è¯„ä¼°æµç¨‹

```python
from app.services.evaluation.adversarial_evaluator import get_adversarial_evaluator

evaluator = get_adversarial_evaluator()

# å¯¹æŠ—æ€§è¯„ä¼°
result = evaluator.adversarial_evaluate(
    content="åˆå§‹å†…å®¹",
    task_context={"name": "ä»»åŠ¡åç§°"},
    max_rounds=3,
    improvement_threshold=0.1
)

print(f"æœ€ä½³å†…å®¹: {result['best_content']}")
print(f"é²æ£’æ€§è¯„åˆ†: {result['best_robustness_score']:.3f}")
print(f"å®Œæˆè½®æ•°: {result['rounds_completed']}")
print(f"å¯¹æŠ—æ€§æ•ˆæœ: {result['final_assessment']['adversarial_effectiveness']:.3f}")
```

### å…ƒè®¤çŸ¥è¯„ä¼°ç³»ç»Ÿ

å…ƒè®¤çŸ¥è¯„ä¼°ç³»ç»Ÿå¯¹è¯„ä¼°è¿‡ç¨‹æœ¬èº«è¿›è¡Œè¯„ä¼°ï¼Œæ£€æµ‹è®¤çŸ¥åè§å’Œè¯„ä¼°è´¨é‡ï¼š

#### è®¤çŸ¥åè§æ£€æµ‹

- **é”šå®šåè§**: è¿‡åº¦ä¾èµ–åˆå§‹ä¿¡æ¯
- **ç¡®è®¤åè§**: å€¾å‘äºç¡®è®¤æ—¢æœ‰è§‚ç‚¹
- **å…‰ç¯æ•ˆåº”**: æ•´ä½“å°è±¡å½±å“å…·ä½“åˆ¤æ–­
- **è¿‘å› æ•ˆåº”**: è¿‡åº¦é‡è§†æœ€è¿‘ä¿¡æ¯
- **ä¸¥é‡æ€§åè§**: å¯¹è´Ÿé¢ä¿¡æ¯è¿‡åº¦æ•æ„Ÿ

#### ä½¿ç”¨ç¤ºä¾‹

```python
from app.services.evaluation.meta_evaluator import get_meta_evaluator

meta_evaluator = get_meta_evaluator()

# å…ƒè®¤çŸ¥è¯„ä¼°
result = meta_evaluator.meta_evaluate_assessment_quality(
    evaluation_history=[eval1, eval2, eval3],
    task_context={"name": "ä»»åŠ¡åç§°"},
    current_evaluation=current_eval
)

print(f"è¯„ä¼°è´¨é‡è¯„åˆ†: {result['assessment_quality_score']:.3f}")
print(f"ä¸€è‡´æ€§è¯„åˆ†: {result['consistency_score']:.3f}")

# æŸ¥çœ‹è®¤çŸ¥åè§é£é™©
bias_risks = result['cognitive_bias_analysis']
for bias_type, risk_level in bias_risks.items():
    if risk_level > 0.3:  # é«˜é£é™©é˜ˆå€¼
        print(f"æ£€æµ‹åˆ° {bias_type} åè§é£é™©: {risk_level:.2f}")
```

### å™¬èŒä½“ä¸“ä¸šè¯„ä¼°å™¨

ä¸“é—¨é’ˆå¯¹å™¬èŒä½“ç ”ç©¶é¢†åŸŸçš„ä¸“ä¸šè¯„ä¼°å™¨ï¼š

#### è¯„ä¼°ç»´åº¦

- **ä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§**: å™¬èŒä½“ç›¸å…³æœ¯è¯­çš„æ­£ç¡®ä½¿ç”¨
- **ä¸´åºŠç›¸å…³æ€§**: ä¸ä¸´åºŠåº”ç”¨çš„ç›¸å…³ç¨‹åº¦
- **å®‰å…¨æ€§è¯„ä¼°**: ç”Ÿç‰©å®‰å…¨å’Œç›‘ç®¡åˆè§„æ€§
- **ç ”ç©¶æ–¹æ³•**: å®éªŒè®¾è®¡å’Œç ”ç©¶æ–¹æ³•çš„ç§‘å­¦æ€§

#### ä½¿ç”¨ç¤ºä¾‹

```python
from app.services.evaluation.phage_evaluator import get_phage_evaluator

phage_evaluator = get_phage_evaluator()

# å™¬èŒä½“ä¸“ä¸šè¯„ä¼°
result = phage_evaluator.evaluate_phage_content(
    content="å™¬èŒä½“ç ”ç©¶å†…å®¹",
    task_context={"research_focus": "therapeutic_applications"}
)

print(f"ä¸“ä¸šè¯„ä¼°è¯„åˆ†: {result['overall_score']:.3f}")
print(f"æœ¯è¯­å‡†ç¡®æ€§: {result['terminology_accuracy']:.3f}")
print(f"ä¸´åºŠç›¸å…³æ€§: {result['clinical_relevance']:.3f}")
print(f"å®‰å…¨æ€§è¯„ä¼°: {result['safety_assessment']:.3f}")
```

### ç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–

ç³»ç»Ÿæä¾›å¤šå±‚ç¼“å­˜æœºåˆ¶ä»¥æå‡æ€§èƒ½ï¼š

#### ç¼“å­˜é…ç½®

```python
from app.services.evaluation.evaluation_cache import get_evaluation_cache

cache = get_evaluation_cache()

# æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
stats = cache.get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
print(f"ç¼“å­˜å¤§å°: {stats['cache_size']} æ¡ç›®")

# ä¼˜åŒ–ç¼“å­˜
optimization_result = cache.optimize_cache()
print(f"æ¸…ç†äº† {optimization_result['entries_removed']} ä¸ªè¿‡æœŸæ¡ç›®")

# æ¸…ç©ºç¼“å­˜
cache.clear_cache()
```

#### æ€§èƒ½ç›‘æ§

```python
# æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
from app.services.evaluation.evaluation_cache import get_evaluation_cache

cache = get_evaluation_cache()
performance_stats = cache.get_performance_stats()

print(f"å¹³å‡æŸ¥è¯¢æ—¶é—´: {performance_stats['avg_query_time']:.3f}ms")
print(f"ç¼“å­˜æ•ˆç‡: {performance_stats['cache_efficiency']:.1%}")
```

### ç›‘ç£ç³»ç»Ÿ

è‡ªç›‘ç£ç³»ç»ŸæŒç»­ç›‘æ§è¯„ä¼°è´¨é‡å¹¶æä¾›è‡ªåŠ¨æ ¡å‡†ï¼š

#### ç›‘ç£æŒ‡æ ‡

- **å‡†ç¡®æ€§**: è¯„ä¼°ç»“æœçš„å‡†ç¡®ç¨‹åº¦
- **ä¸€è‡´æ€§**: è¯„ä¼°ç»“æœçš„ä¸€è‡´æ€§
- **æ€§èƒ½**: è¯„ä¼°æ‰§è¡Œçš„æ€§èƒ½è¡¨ç°
- **ç¼“å­˜æ•ˆç‡**: ç¼“å­˜ç³»ç»Ÿçš„æ•ˆç‡
- **ç½®ä¿¡åº¦**: è¯„ä¼°ç»“æœçš„ç½®ä¿¡æ°´å¹³

#### ç›‘ç£é…ç½®

```python
from app.services.evaluation.evaluation_supervisor import get_evaluation_supervisor

supervisor = get_evaluation_supervisor()

# æ›´æ–°ç›‘ç£é˜ˆå€¼
new_thresholds = {
    "min_accuracy": 0.75,
    "min_consistency": 0.65,
    "max_evaluation_time": 25.0
}
supervisor.update_thresholds(new_thresholds)

# è·å–ç›‘ç£æŠ¥å‘Š
report = supervisor.get_supervision_report()
print(f"ç³»ç»Ÿå¥åº·è¯„åˆ†: {report['system_health']['overall_score']:.3f}")
print(f"ç³»ç»ŸçŠ¶æ€: {report['system_health']['status']}")
```

## æ‰¹é‡å¤„ç†

### æ‰¹é‡è¯„ä¼°

```bash
# æ‰¹é‡è¯„ä¼°å¤šä¸ªä»»åŠ¡
python -m cli.main --eval-batch --task-ids 123,124,125 --threshold 0.8 --max-iterations 3
```

### æ‰¹é‡é…ç½®

```python
# æ‰¹é‡é…ç½®è¯„ä¼°å‚æ•°
task_ids = [123, 124, 125]
for task_id in task_ids:
    default_repo.store_evaluation_config(
        task_id=task_id,
        quality_threshold=0.8,
        max_iterations=3,
        strict_mode=True
    )
```

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ¨¡å¼

- **ç®€å•å†…å®¹**: ä½¿ç”¨åŸºç¡€è¯„ä¼°æˆ–LLMæ™ºèƒ½è¯„ä¼°
- **ä¸“ä¸šå†…å®¹**: ä½¿ç”¨å¤šä¸“å®¶è¯„ä¼°æˆ–é¢†åŸŸä¸“ä¸šè¯„ä¼°
- **é«˜è´¨é‡è¦æ±‚**: ä½¿ç”¨å¯¹æŠ—æ€§è¯„ä¼°
- **è´¨é‡ç›‘æ§**: å¯ç”¨ç›‘ç£ç³»ç»Ÿ

### 2. ä¼˜åŒ–æ€§èƒ½

- å¯ç”¨ç¼“å­˜ä»¥å‡å°‘é‡å¤è®¡ç®—
- åˆç†è®¾ç½®è´¨é‡é˜ˆå€¼é¿å…è¿‡åº¦è¿­ä»£
- å®šæœŸæ¸…ç†ç¼“å­˜å’Œä¼˜åŒ–ç³»ç»Ÿ
- ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡

### 3. è´¨é‡æ§åˆ¶

- è®¾ç½®åˆç†çš„è´¨é‡é˜ˆå€¼ï¼ˆæ¨è0.7-0.8ï¼‰
- é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆæ¨è3-5æ¬¡ï¼‰
- å¯ç”¨ç›‘ç£ç³»ç»Ÿè¿›è¡Œè´¨é‡ç›‘æ§
- å®šæœŸæŸ¥çœ‹è¯„ä¼°å†å²å’Œç»Ÿè®¡ä¿¡æ¯

### 4. æ•…éšœæ’é™¤

```bash
# æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€
python -m cli.main --eval-supervision

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
python -m cli.main --eval-stats --detailed

# é‡ç½®ç›‘ç£çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
python -c "from app.services.evaluation.evaluation_supervisor import get_evaluation_supervisor; get_evaluation_supervisor().reset_supervision_state()"

# æ¸…ç†ç¼“å­˜
python -c "from app.services.evaluation.evaluation_cache import get_evaluation_cache; get_evaluation_cache().clear_cache()"
```

## APIå‚è€ƒ

### ä¸»è¦æ‰§è¡Œå‡½æ•°

```python
# åŸºç¡€è¯„ä¼°
execute_task_with_evaluation(task, repo, max_iterations, quality_threshold, evaluation_config, use_context, context_options)

# LLMæ™ºèƒ½è¯„ä¼°
execute_task_with_llm_evaluation(task, repo, max_iterations, quality_threshold, evaluation_config, use_context, context_options)

# å¤šä¸“å®¶è¯„ä¼°
execute_task_with_multi_expert_evaluation(task, repo, max_iterations, quality_threshold, selected_experts, evaluation_config, use_context, context_options)

# å¯¹æŠ—æ€§è¯„ä¼°
execute_task_with_adversarial_evaluation(task, repo, max_rounds, improvement_threshold, evaluation_config, use_context, context_options)
```

### é…ç½®ç±»

```python
from app.models import EvaluationConfig

config = EvaluationConfig(
    quality_threshold=0.8,           # è´¨é‡é˜ˆå€¼
    max_iterations=3,                # æœ€å¤§è¿­ä»£æ¬¡æ•°
    strict_mode=True,                # ä¸¥æ ¼æ¨¡å¼
    evaluation_dimensions=[...],     # è¯„ä¼°ç»´åº¦
    domain_specific=False,           # é¢†åŸŸç‰¹å®šè¯„ä¼°
    custom_weights={...}             # è‡ªå®šä¹‰æƒé‡
)
```

## æ‰©å±•å’Œå®šåˆ¶

### æ·»åŠ æ–°çš„è¯„ä¼°å™¨

1. ç»§æ‰¿åŸºç¡€è¯„ä¼°å™¨ç±»
2. å®ç°è¯„ä¼°é€»è¾‘
3. æ³¨å†Œåˆ°æ‰§è¡Œç³»ç»Ÿ
4. æ·»åŠ CLIæ”¯æŒ

### è‡ªå®šä¹‰ä¸“å®¶

```python
# åœ¨expert_evaluator.pyä¸­æ·»åŠ æ–°ä¸“å®¶
new_expert = {
    "name": "custom_expert",
    "role": "è‡ªå®šä¹‰ä¸“å®¶",
    "expertise": ["ä¸“ä¸šé¢†åŸŸ1", "ä¸“ä¸šé¢†åŸŸ2"],
    "evaluation_focus": ["å…³æ³¨ç‚¹1", "å…³æ³¨ç‚¹2"],
    "evaluation_criteria": {
        "criterion1": "æ ‡å‡†1æè¿°",
        "criterion2": "æ ‡å‡†2æè¿°"
    }
}
```

### è‡ªå®šä¹‰ç›‘ç£æŒ‡æ ‡

```python
# åœ¨evaluation_supervisor.pyä¸­æ·»åŠ æ–°æŒ‡æ ‡
def _calculate_custom_metric(self, evaluation_result, execution_time):
    # å®ç°è‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—é€»è¾‘
    return metric_value
```

## å¸¸è§é—®é¢˜

### Q: è¯„ä¼°é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A:

1. å¯ç”¨ç¼“å­˜ç³»ç»Ÿ
2. é™ä½è´¨é‡é˜ˆå€¼
3. å‡å°‘æœ€å¤§è¿­ä»£æ¬¡æ•°
4. ä½¿ç”¨æ›´ç®€å•çš„è¯„ä¼°æ¨¡å¼

### Q: è¯„ä¼°ç»“æœä¸ä¸€è‡´æ€ä¹ˆåŠï¼Ÿ
A:

1. æ£€æŸ¥ç›‘ç£ç³»ç»ŸæŠ¥å‘Š
2. æŸ¥çœ‹ä¸€è‡´æ€§æŒ‡æ ‡
3. è€ƒè™‘ä½¿ç”¨å¤šä¸“å®¶è¯„ä¼°
4. å¯ç”¨ä¸¥æ ¼æ¨¡å¼

### Q: å¦‚ä½•æé«˜è¯„ä¼°å‡†ç¡®æ€§ï¼Ÿ
A:

1. ä½¿ç”¨LLMæ™ºèƒ½è¯„ä¼°
2. å¯ç”¨å¤šä¸“å®¶è¯„ä¼°
3. å¢åŠ è¯„ä¼°ç»´åº¦
4. ä½¿ç”¨é¢†åŸŸä¸“ä¸šè¯„ä¼°å™¨

### Q: ç³»ç»Ÿèµ„æºå ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ
A:

1. å®šæœŸæ¸…ç†ç¼“å­˜
2. ä¼˜åŒ–ç¼“å­˜é…ç½®
3. é™åˆ¶å¹¶å‘è¯„ä¼°æ•°é‡
4. ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡

## æ›´æ–°æ—¥å¿—

### v2.0.0 (å½“å‰ç‰ˆæœ¬)

- âœ… æ–°å¢LLMæ™ºèƒ½è¯„ä¼°å™¨
- âœ… æ–°å¢å¤šä¸“å®¶è¯„ä¼°ç³»ç»Ÿ
- âœ… æ–°å¢å¯¹æŠ—æ€§è¯„ä¼°æœºåˆ¶
- âœ… æ–°å¢å…ƒè®¤çŸ¥è¯„ä¼°ç³»ç»Ÿ
- âœ… æ–°å¢å™¬èŒä½“ä¸“ä¸šè¯„ä¼°å™¨
- âœ… æ–°å¢å¤šå±‚ç¼“å­˜ç³»ç»Ÿ
- âœ… æ–°å¢è‡ªç›‘ç£è´¨é‡æ§åˆ¶
- âœ… å®Œå–„CLIå‘½ä»¤æ”¯æŒ
- âœ… æ–°å¢æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

### v1.0.0

- âœ… åŸºç¡€è¯„ä¼°ç³»ç»Ÿ
- âœ… è¯„ä¼°å†å²ç®¡ç†
- âœ… åŸºç¡€CLIæ”¯æŒ

## æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æŸ¥çœ‹ï¼š

1. ç³»ç»Ÿç›‘ç£æŠ¥å‘Š: `python -m cli.main --eval-supervision --detailed`
2. è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯: `python -m cli.main --eval-stats --detailed`
3. é”™è¯¯æ—¥å¿—å’Œè°ƒè¯•ä¿¡æ¯

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œæœ€åæ›´æ–°æ—¶é—´: 2024å¹´*

# AI-Driven æ™ºèƒ½ä»»åŠ¡ç¼–æ’ç³»ç»Ÿ API å‚è€ƒ

## ğŸš€ é€’å½’ä»»åŠ¡åˆ†è§£ API

### ä»»åŠ¡åˆ†è§£ç«¯ç‚¹

#### POST /tasks/{task_id}/decompose
å¯¹æŒ‡å®šä»»åŠ¡è¿›è¡Œæ™ºèƒ½åˆ†è§£ã€‚

**è¯·æ±‚å‚æ•°:**
```json
{
  "max_subtasks": 5,      // æœ€å¤§å­ä»»åŠ¡æ•°é‡ (2-20ï¼Œé»˜è®¤8)
  "force": false          // å¼ºåˆ¶åˆ†è§£ï¼Œå¿½ç•¥ç°æœ‰å­ä»»åŠ¡
}
```

**å“åº”ç¤ºä¾‹:**
```json
{
  "success": true,
  "task_id": 123,
  "subtasks": [
    {
      "id": 124,
      "name": "ç”¨æˆ·æ³¨å†Œæ¨¡å—",
      "type": "composite",
      "priority": 100
    }
  ],
  "decomposition_depth": 1
}
```

#### POST /tasks/{task_id}/decompose/with-evaluation
å¸¦è´¨é‡è¯„ä¼°çš„ä»»åŠ¡åˆ†è§£ï¼Œæ”¯æŒè¿­ä»£æ”¹è¿›ã€‚

**è¯·æ±‚å‚æ•°:**
```json
{
  "max_subtasks": 5,
  "quality_threshold": 0.7,    // è´¨é‡é˜ˆå€¼ (0.0-1.0)
  "max_iterations": 2          // æœ€å¤§è¿­ä»£æ¬¡æ•°
}
```

**å“åº”ç¤ºä¾‹:**
```json
{
  "success": true,
  "task_id": 123,
  "subtasks": [...],
  "quality_evaluation": {
    "quality_score": 0.85,
    "needs_refinement": false,
    "issues": [],
    "suggestions": []
  },
  "best_quality_score": 0.85,
  "meets_threshold": true,
  "iterations_performed": 1
}
```

#### GET /tasks/{task_id}/complexity
è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦ã€‚

**å“åº”ç¤ºä¾‹:**
```json
{
  "task_id": 123,
  "complexity": "high",           // high/medium/low
  "task_type": "root",           // root/composite/atomic
  "should_decompose": true,
  "depth": 0,
  "existing_children": 0
}
```

#### GET /tasks/{task_id}/decomposition/recommendation
è·å–ä»»åŠ¡åˆ†è§£å»ºè®®ã€‚

**è¯·æ±‚å‚æ•°:**
- `min_complexity_score`: æœ€å°å¤æ‚åº¦åˆ†æ•° (é»˜è®¤0.6)

**å“åº”ç¤ºä¾‹:**
```json
{
  "task_id": 123,
  "recommendation": {
    "should_decompose": true,
    "complexity": "high",
    "complexity_score": 0.9,
    "recommendations": [
      "ä»»åŠ¡å¤æ‚åº¦è¾ƒé«˜ï¼Œå»ºè®®è¿›è¡Œåˆ†è§£",
      "å»ºè®®åˆ†è§£ä¸º4-6ä¸ªå­ä»»åŠ¡"
    ]
  },
  "analysis": {
    "basic_decomposition_eligible": true,
    "complexity_sufficient": true,
    "within_depth_limit": true,
    "not_atomic": true
  },
  "timestamp": "2024-08-31T10:30:00Z"
}
```

#### POST /plans/{title}/decompose
é€’å½’åˆ†è§£æ•´ä¸ªè®¡åˆ’ä¸­çš„æ‰€æœ‰ä»»åŠ¡ã€‚

**è¯·æ±‚å‚æ•°:**
```json
{
  "max_depth": 3    // æœ€å¤§åˆ†è§£æ·±åº¦
}
```

**å“åº”ç¤ºä¾‹:**
```json
{
  "success": true,
  "plan_title": "æ™ºèƒ½ç³»ç»Ÿå¼€å‘è®¡åˆ’",
  "decompositions": [...],
  "total_tasks_decomposed": 5
}
```

### ä»»åŠ¡åˆ†è§£ç®—æ³•è¯´æ˜

#### å¤æ‚åº¦è¯„ä¼°ç®—æ³•
åŸºäºå…³é”®è¯å¯†åº¦å’Œä»»åŠ¡æè¿°é•¿åº¦è¿›è¡Œæ™ºèƒ½è¯„ä¼°ï¼š

**é«˜å¤æ‚åº¦å…³é”®è¯:**
- ç³»ç»Ÿã€æ¶æ„ã€å¹³å°ã€æ¡†æ¶ã€å®Œæ•´ã€å…¨é¢ã€ç«¯åˆ°ç«¯ã€æ•´ä½“ã€ç»¼åˆ

**ä¸­ç­‰å¤æ‚åº¦å…³é”®è¯:**
- æ¨¡å—ã€ç»„ä»¶ã€åŠŸèƒ½ã€ç‰¹æ€§ã€é›†æˆã€ä¼˜åŒ–ã€é‡æ„ã€æ‰©å±•

**ä½å¤æ‚åº¦å…³é”®è¯:**
- ä¿®å¤ã€è°ƒè¯•ã€æµ‹è¯•ã€æ–‡æ¡£ã€é…ç½®ã€éƒ¨ç½²ã€æ›´æ–°ã€æ£€æŸ¥

#### ä»»åŠ¡ç±»å‹ä½“ç³»
```
ROOT (æ·±åº¦0)     â†’ COMPOSITE (æ·±åº¦1)  â†’ ATOMIC (æ·±åº¦2)
é«˜å¤æ‚åº¦é¡¹ç›®      â†’ ä¸­ç­‰ç²’åº¦ä»»åŠ¡        â†’ å¯æ‰§è¡Œæœ€å°å•å…ƒ
```

#### è´¨é‡è¯„ä¼°æŒ‡æ ‡
- **å­ä»»åŠ¡æ•°é‡**: 2-8ä¸ªä¸ºæœ€ä¼˜
- **åç§°è´¨é‡**: é¿å…ç©ºåç§°å’Œæ³›åŒ–åç§°
- **ç±»å‹ä¸€è‡´æ€§**: åŒå±‚çº§ä»»åŠ¡ç±»å‹åº”ä¿æŒä¸€è‡´
- **é‡å æ£€æµ‹**: é¿å…å­ä»»åŠ¡é—´åŠŸèƒ½é‡å 

## ğŸ¯ è¯„ä¼°ç³»ç»Ÿ API

### æ ¸å¿ƒæ‰§è¡Œå‡½æ•°

### execute_task_with_evaluation()

åŸºç¡€è¯„ä¼°æ‰§è¡Œå‡½æ•°ã€‚

```python
def execute_task_with_evaluation(
    task,
    repo: Optional[TaskRepository] = None,
    max_iterations: int = 3,
    quality_threshold: float = 0.8,
    evaluation_config: Optional[EvaluationConfig] = None,
    use_context: bool = False,
    context_options: Optional[Dict[str, Any]] = None,
) -> TaskExecutionResult
```

**å‚æ•°:**
- `task`: ä»»åŠ¡å¯¹è±¡æˆ–å­—å…¸
- `repo`: ä»»åŠ¡ä»“åº“å®ä¾‹
- `max_iterations`: æœ€å¤§è¿­ä»£æ¬¡æ•°
- `quality_threshold`: è´¨é‡é˜ˆå€¼ (0.0-1.0)
- `evaluation_config`: è¯„ä¼°é…ç½®å¯¹è±¡
- `use_context`: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡
- `context_options`: ä¸Šä¸‹æ–‡é€‰é¡¹

**è¿”å›:** `TaskExecutionResult` å¯¹è±¡

### execute_task_with_llm_evaluation()

LLMæ™ºèƒ½è¯„ä¼°æ‰§è¡Œå‡½æ•°ã€‚

```python
def execute_task_with_llm_evaluation(
    task,
    repo: Optional[TaskRepository] = None,
    max_iterations: int = 3,
    quality_threshold: float = 0.8,
    evaluation_config: Optional[EvaluationConfig] = None,
    use_context: bool = False,
    context_options: Optional[Dict[str, Any]] = None,
) -> TaskExecutionResult
```

**ç‰¹ç‚¹:**
- ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­ä¹‰ç†è§£
- æä¾›6ä¸ªç»´åº¦çš„è¯¦ç»†è¯„ä¼°
- æ™ºèƒ½ç”Ÿæˆæ”¹è¿›å»ºè®®

### execute_task_with_multi_expert_evaluation()

å¤šä¸“å®¶è¯„ä¼°æ‰§è¡Œå‡½æ•°ã€‚

```python
def execute_task_with_multi_expert_evaluation(
    task,
    repo: Optional[TaskRepository] = None,
    max_iterations: int = 3,
    quality_threshold: float = 0.8,
    selected_experts: Optional[List[str]] = None,
    evaluation_config: Optional[EvaluationConfig] = None,
    use_context: bool = False,
    context_options: Optional[Dict[str, Any]] = None,
) -> TaskExecutionResult
```

**å‚æ•°:**
- `selected_experts`: é€‰æ‹©çš„ä¸“å®¶åˆ—è¡¨ï¼Œå¯é€‰å€¼:
  - `"theoretical_biologist"`: ç†è®ºç”Ÿç‰©å­¦å®¶
  - `"clinical_physician"`: ä¸´åºŠåŒ»å¸ˆ
  - `"regulatory_expert"`: ç›‘ç®¡ä¸“å®¶
  - `"research_scientist"`: ç ”ç©¶ç§‘å­¦å®¶
  - `"biotech_entrepreneur"`: ç”Ÿç‰©æŠ€æœ¯ä¼ä¸šå®¶

### execute_task_with_adversarial_evaluation()

å¯¹æŠ—æ€§è¯„ä¼°æ‰§è¡Œå‡½æ•°ã€‚

```python
def execute_task_with_adversarial_evaluation(
    task,
    repo: Optional[TaskRepository] = None,
    max_rounds: int = 3,
    improvement_threshold: float = 0.1,
    evaluation_config: Optional[EvaluationConfig] = None,
    use_context: bool = False,
    context_options: Optional[Dict[str, Any]] = None,
) -> TaskExecutionResult
```

**å‚æ•°:**
- `max_rounds`: æœ€å¤§å¯¹æŠ—è½®æ•°
- `improvement_threshold`: æ”¹è¿›é˜ˆå€¼

## é…ç½®ç±»

### EvaluationConfig

è¯„ä¼°é…ç½®ç±»ï¼ˆPydanticï¼‰ã€‚

```python
class EvaluationConfig(BaseModel):
    quality_threshold: float = 0.8
    max_iterations: int = 3
    evaluation_dimensions: List[str] = [
        "relevance", "completeness", "accuracy", "clarity", "coherence"
    ]
    domain_specific: bool = False
    strict_mode: bool = False
    custom_weights: Optional[Dict[str, float]] = None
```

**å­—æ®µè¯´æ˜:**
- `quality_threshold`: è´¨é‡é˜ˆå€¼ (0.0-1.0)
- `max_iterations`: æœ€å¤§è¿­ä»£æ¬¡æ•°
- `strict_mode`: ä¸¥æ ¼æ¨¡å¼ï¼Œå¯ç”¨æ›´ä¸¥æ ¼çš„è¯„ä¼°æ ‡å‡†
- `evaluation_dimensions`: è¯„ä¼°ç»´åº¦åˆ—è¡¨
- `domain_specific`: æ˜¯å¦å¯ç”¨é¢†åŸŸç‰¹å®šè¯„ä¼°
- `custom_weights`: è‡ªå®šä¹‰ç»´åº¦æƒé‡

**è¯„ä¼°ç»´åº¦é€‰é¡¹:**
- `"relevance"`: ç›¸å…³æ€§
- `"completeness"`: å®Œæ•´æ€§
- `"accuracy"`: å‡†ç¡®æ€§
- `"clarity"`: æ¸…æ™°åº¦
- `"coherence"`: è¿è´¯æ€§
- `"scientific_rigor"`: ç§‘å­¦ä¸¥è°¨æ€§

### TaskExecutionResult

ä»»åŠ¡æ‰§è¡Œç»“æœç±»ï¼ˆPydanticï¼‰ã€‚

```python
class TaskExecutionResult(BaseModel):
    task_id: int
    status: str
    content: Optional[str] = None
    evaluation: Optional[EvaluationResult] = None
    iterations: int = 1
    execution_time: Optional[float] = None
```

**å­—æ®µè¯´æ˜:**
- `task_id`: ä»»åŠ¡ID
- `status`: æ‰§è¡ŒçŠ¶æ€ ("done", "needs_review", "failed")
- `content`: ç”Ÿæˆçš„å†…å®¹
- `evaluation`: è¯„ä¼°ç»“æœ
- `iterations`: å®Œæˆçš„è¿­ä»£æ¬¡æ•°
- `execution_time`: æ‰§è¡Œæ—¶é—´(ç§’)
- `metadata`: é¢å¤–å…ƒæ•°æ®

### EvaluationResult

è¯„ä¼°ç»“æœç±»ï¼ˆPydanticï¼‰ã€‚

```python
class EvaluationResult(BaseModel):
    overall_score: float
    dimensions: EvaluationDimensions
    suggestions: List[str] = []
    needs_revision: bool = False
    iteration: int = 0
    timestamp: Optional[datetime] = None
    metadata: Optional[Dict[str, Any]] = None
```

## è¯„ä¼°å™¨ç±»

### LLMEvaluator

LLMæ™ºèƒ½è¯„ä¼°å™¨ã€‚

```python
class LLMEvaluator:
    def __init__(self, config: Optional[EvaluationConfig] = None)
    
    def evaluate_content_intelligent(
        self, 
        content: str, 
        task_context: Dict[str, Any], 
        iteration: int = 1
    ) -> EvaluationResult
```

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from app.services.evaluation.llm_evaluator import get_llm_evaluator

evaluator = get_llm_evaluator()
result = evaluator.evaluate_content_intelligent(
    content="å¾…è¯„ä¼°å†…å®¹",
    task_context={"name": "ä»»åŠ¡åç§°", "task_type": "content_generation"},
    iteration=1
)
```

## Benchmark åŸºå‡†è¯„æµ‹æ¥å£

### REST API

```http
POST /benchmark
Content-Type: application/json

{
  "topic": "æŠ—èŒç´ è€è¯",
  "configs": [
    "base,use_context=False",
    "ctx,use_context=True,max_chars=3000,semantic_k=5"
  ],
  "sections": 5
}
```

è¿”å›ï¼š
- `summary_md`: æ±‡æ€» Markdown è¡¨
- `metrics`: æ¯ä¸ªé…ç½®çš„å‡å€¼ã€ç»´åº¦å‡å€¼ã€å¤±è´¥æ•°ã€è®¡æ•°ç­‰
- `files`: æ¯ä¸ªé…ç½®ç”Ÿæˆçš„ MD æ–‡ä»¶è·¯å¾„ï¼ˆè‹¥è®¾ç½® outdirï¼‰
- `csv_path`: ç»Ÿä¸€ CSV è·¯å¾„ï¼ˆè‹¥è®¾ç½® csv_pathï¼‰

### CLI

```bash
python -m cli.main --benchmark \
  --benchmark-topic "æŠ—èŒç´ è€è¯" \
  --benchmark-configs "base,use_context=False" "ctx,use_context=True,max_chars=3000,semantic_k=5" \
  --benchmark-sections 5 \
  --benchmark-outdir results/æŠ—èŒç´ è€è¯ \
  --benchmark-csv results/æŠ—èŒç´ è€è¯/summary.csv \
  --benchmark-output results/æŠ—èŒç´ è€è¯/overview.md
```

### MultiExpertEvaluator

å¤šä¸“å®¶è¯„ä¼°å™¨ã€‚

```python
class MultiExpertEvaluator:
    def __init__(self, config: Optional[EvaluationConfig] = None)
    
    def evaluate_with_multiple_experts(
        self,
        content: str,
        task_context: Dict[str, Any],
        selected_experts: Optional[List[str]] = None,
        iteration: int = 1
    ) -> Dict[str, Any]
```

**è¿”å›ç»“æœç»“æ„:**
```python
{
    "expert_evaluations": {
        "expert_name": {
            "overall_score": float,
            "expert_role": str,
            "confidence_level": float,
            "major_concerns": List[str],
            "specific_suggestions": List[str]
        }
    },
    "consensus": {
        "overall_score": float,
        "consensus_confidence": float,
        "specific_suggestions": List[str]
    },
    "disagreements": [
        {
            "field": str,
            "disagreement_level": float,
            "lowest_scorer": str,
            "highest_scorer": str
        }
    ],
    "metadata": Dict[str, Any]
}
```

### AdversarialEvaluator

å¯¹æŠ—æ€§è¯„ä¼°å™¨ã€‚

```python
class AdversarialEvaluator:
    def __init__(self, config: Optional[EvaluationConfig] = None)
    
    def adversarial_evaluate(
        self,
        content: str,
        task_context: Dict[str, Any],
        max_rounds: int = 3,
        improvement_threshold: float = 0.1
    ) -> Dict[str, Any]
```

### MetaEvaluator

å…ƒè®¤çŸ¥è¯„ä¼°å™¨ã€‚

```python
class MetaEvaluator:
    def __init__(self, config: Optional[EvaluationConfig] = None)
    
    def meta_evaluate_assessment_quality(
        self,
        evaluation_history: List[Dict[str, Any]],
        task_context: Dict[str, Any],
        current_evaluation: Dict[str, Any]
    ) -> Dict[str, Any]
```

### PhageEvaluator

å™¬èŒä½“ä¸“ä¸šè¯„ä¼°å™¨ã€‚

```python
class PhageEvaluator:
    def __init__(self, config: Optional[EvaluationConfig] = None)
    
    def evaluate_phage_content(
        self,
        content: str,
        task_context: Dict[str, Any]
    ) -> Dict[str, Any]
```

## ç¼“å­˜ç³»ç»Ÿ

### EvaluationCache

è¯„ä¼°ç¼“å­˜ç®¡ç†å™¨ã€‚

```python
class EvaluationCache:
    def get_cache_stats(self) -> Dict[str, Any]
    def optimize_cache(self) -> Dict[str, Any]
    def clear_cache(self) -> bool
    def get_performance_stats(self) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from app.services.evaluation.evaluation_cache import get_evaluation_cache

cache = get_evaluation_cache()

# è·å–ç¼“å­˜ç»Ÿè®¡
stats = cache.get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")

# ä¼˜åŒ–ç¼“å­˜
optimization_result = cache.optimize_cache()
print(f"æ¸…ç†äº† {optimization_result['entries_removed']} ä¸ªæ¡ç›®")
```

## ç›‘ç£ç³»ç»Ÿ

### EvaluationSupervisor

è¯„ä¼°è´¨é‡ç›‘ç£å™¨ã€‚

```python
class EvaluationSupervisor:
    def monitor_evaluation(
        self, 
        evaluation_result: EvaluationResult,
        evaluation_method: str,
        execution_time: float,
        content: str,
        task_context: Dict[str, Any]
    ) -> Dict[str, Any]
    
    def get_supervision_report(self) -> Dict[str, Any]
    
    def update_thresholds(self, new_thresholds: Dict[str, float]) -> bool
    
    def reset_supervision_state(self) -> bool
```

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from app.services.evaluation.evaluation_supervisor import get_evaluation_supervisor

supervisor = get_evaluation_supervisor()

# è·å–ç›‘ç£æŠ¥å‘Š
report = supervisor.get_supervision_report()
print(f"ç³»ç»Ÿå¥åº·è¯„åˆ†: {report['system_health']['overall_score']:.3f}")

# æ›´æ–°ç›‘ç£é˜ˆå€¼
new_thresholds = {
    "min_accuracy": 0.75,
    "max_evaluation_time": 30.0
}
supervisor.update_thresholds(new_thresholds)
```

## å·¥å…·å‡½æ•°

### monitor_evaluation()

ç›‘æ§å•æ¬¡è¯„ä¼°çš„ä¾¿æ·å‡½æ•°ã€‚

```python
def monitor_evaluation(
    evaluation_result: EvaluationResult,
    evaluation_method: str,
    execution_time: float,
    content: str,
    task_context: Dict[str, Any]
) -> Dict[str, Any]
```

### get_supervision_report()

è·å–ç›‘ç£æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°ã€‚

```python
def get_supervision_report() -> Dict[str, Any]
```

## é”™è¯¯å¤„ç†

### å¸¸è§å¼‚å¸¸

```python
# è¯„ä¼°é…ç½®é”™è¯¯
class EvaluationConfigError(Exception):
    pass

# è¯„ä¼°æ‰§è¡Œé”™è¯¯
class EvaluationExecutionError(Exception):
    pass

# ç¼“å­˜é”™è¯¯
class CacheError(Exception):
    pass

# ç›‘ç£ç³»ç»Ÿé”™è¯¯
class SupervisionError(Exception):
    pass
```

### é”™è¯¯å¤„ç†ç¤ºä¾‹

```python
try:
    result = execute_task_with_llm_evaluation(
        task=task,
        quality_threshold=0.8,
        max_iterations=3
    )
except EvaluationExecutionError as e:
    print(f"è¯„ä¼°æ‰§è¡Œå¤±è´¥: {e}")
    # å¤„ç†é”™è¯¯
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
    # é€šç”¨é”™è¯¯å¤„ç†
```

## æœ€ä½³å®è·µ

### 1. é…ç½®ä¼˜åŒ–

```python
# æ¨èçš„é…ç½®
config = EvaluationConfig(
    quality_threshold=0.8,  # é€‚ä¸­çš„è´¨é‡è¦æ±‚
    max_iterations=3,       # é¿å…è¿‡åº¦è¿­ä»£
    strict_mode=True,       # å¯ç”¨ä¸¥æ ¼æ¨¡å¼
    evaluation_dimensions=[
        "relevance", "completeness", "accuracy", 
        "clarity", "coherence", "scientific_rigor"
    ]
)
```

### 2. é”™è¯¯æ¢å¤

```python
def robust_evaluation(task, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„è¯„ä¼°"""
    for attempt in range(max_retries):
        try:
            return execute_task_with_llm_evaluation(task)
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### 3. æ€§èƒ½ç›‘æ§

```python
def monitored_evaluation(task):
    """å¸¦æ€§èƒ½ç›‘æ§çš„è¯„ä¼°"""
    start_time = time.time()
    
    try:
        result = execute_task_with_llm_evaluation(task)
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        execution_time = time.time() - start_time
        if execution_time > 30:  # è¶…è¿‡30ç§’è­¦å‘Š
            print(f"âš ï¸ è¯„ä¼°è€—æ—¶è¾ƒé•¿: {execution_time:.2f}ç§’")
        
        return result
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        raise
```

### 4. æ‰¹é‡å¤„ç†

```python
def batch_evaluation(tasks, batch_size=5):
    """æ‰¹é‡è¯„ä¼°å¤„ç†"""
    results = []
    
    for i in range(0, len(tasks), batch_size):
        batch = tasks[i:i + batch_size]
        
        for task in batch:
            try:
                result = execute_task_with_llm_evaluation(task)
                results.append(result)
            except Exception as e:
                print(f"ä»»åŠ¡ {task.get('id', 'unknown')} è¯„ä¼°å¤±è´¥: {e}")
                results.append(None)
        
        # æ‰¹æ¬¡é—´æš‚åœï¼Œé¿å…è¿‡è½½
        if i + batch_size < len(tasks):
            time.sleep(1)
    
    return results
```

## ç‰ˆæœ¬å…¼å®¹æ€§

### å½“å‰ç‰ˆæœ¬: 2.0.0

**æ–°å¢åŠŸèƒ½:**
- LLMæ™ºèƒ½è¯„ä¼°
- å¤šä¸“å®¶è¯„ä¼°ç³»ç»Ÿ
- å¯¹æŠ—æ€§è¯„ä¼°æœºåˆ¶
- å…ƒè®¤çŸ¥è¯„ä¼°
- å™¬èŒä½“ä¸“ä¸šè¯„ä¼°
- è‡ªç›‘ç£è´¨é‡æ§åˆ¶
- å¤šå±‚ç¼“å­˜ç³»ç»Ÿ

**å‘åå…¼å®¹:**
- ä¿æŒä¸1.xç‰ˆæœ¬çš„APIå…¼å®¹
- æ—§çš„è¯„ä¼°å‡½æ•°ä»ç„¶å¯ç”¨
- é…ç½®å‚æ•°å‘åå…¼å®¹

### è¿ç§»æŒ‡å—

ä»1.xç‰ˆæœ¬è¿ç§»åˆ°2.0ç‰ˆæœ¬:

```python
# æ—§ç‰ˆæœ¬ (1.x)
result = execute_task(task, enable_evaluation=True)

# æ–°ç‰ˆæœ¬ (2.0) - æ¨è
result = execute_task_with_llm_evaluation(task)

# æˆ–è€…ä¿æŒå…¼å®¹
result = execute_task(task, enable_evaluation=True)  # ä»ç„¶æœ‰æ•ˆ
```

## æ‰©å±•å¼€å‘

### è‡ªå®šä¹‰è¯„ä¼°å™¨

```python
from app.services.evaluation.content_evaluator import ContentEvaluator

class CustomEvaluator(ContentEvaluator):
    def __init__(self, config: Optional[EvaluationConfig] = None):
        super().__init__(config)
    
    def evaluate_content(
        self, 
        content: str, 
        task_context: Dict[str, Any], 
        iteration: int = 1
    ) -> EvaluationResult:
        # å®ç°è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
        pass
```

### è‡ªå®šä¹‰ä¸“å®¶

```python
# åœ¨expert_evaluator.pyä¸­æ·»åŠ 
CUSTOM_EXPERT = {
    "name": "custom_expert",
    "role": "è‡ªå®šä¹‰ä¸“å®¶",
    "expertise": ["ä¸“ä¸šé¢†åŸŸ"],
    "evaluation_focus": ["å…³æ³¨ç‚¹"],
    "evaluation_criteria": {
        "criterion1": "è¯„ä¼°æ ‡å‡†1"
    }
}
```

---

*APIå‚è€ƒæ–‡æ¡£ v2.0.0 - æœ€åæ›´æ–°: 2024å¹´*
